{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMZ - Amazon reviews summarization chrome extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Here we're cleaning up the Amazon reviews dataset from [Kaggle.](https://www.kaggle.com/snap/amazon-fine-food-reviews)\n",
    "\n",
    "It has 568,454 food reviews on Amazon up to October 2012, with the following columns in Reviews.csv:\n",
    "\n",
    "\n",
    "| Field        | Description\n",
    "|:------------- |:-------------\n",
    "| Id      | ID of review\n",
    "| ProductId      | unique identifier for the product\n",
    "| UserId | unqiue identifier for the user\n",
    "| ProfileName | -- \n",
    "| HelpfulnessNumerator | number of users who found the review helpful\n",
    "| HelpfulnessDenominator | number of users who indicated whether they found the review helpful\n",
    "| Score | rating between 1 and 5\n",
    "| Time | timestamp for the review  \n",
    "| Summary | brief summary of the review  \n",
    "| Text | text of the review\n",
    "\n",
    "The only columns that we care about are <b>Text</b> and <b>Summary</b>; our motivation is to use the text-summary pairs to train our sequence-to-sequence model to generate its own summaries given a review text (which we'll be scraping from the Amazon product page).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from helpers import text_cleaning\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drop rows with any NAs,\n",
    "## Drop columns except 'Summary' and 'Text'\n",
    "reviews = reviews.dropna()\n",
    "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n",
    "                        'Score','Time'], 1)\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into some of the reviews and see their summary pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### REVIEW TEXT 1:\n",
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "### SUMMARY 1:Good Quality Dog Food\n",
      "\n",
      "### REVIEW TEXT 2:\n",
      "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "### SUMMARY 2:Not as Advertised\n",
      "\n",
      "### REVIEW TEXT 3:\n",
      "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "### SUMMARY 3:\"Delight\" says it all\n",
      "\n",
      "### REVIEW TEXT 4:\n",
      "If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "### SUMMARY 4:Cough Medicine\n",
      "\n",
      "### REVIEW TEXT 5:\n",
      "Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "### SUMMARY 5:Great taffy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_to_inspect = 5\n",
    "for i in range(reviews_to_inspect):\n",
    "    print(\"### REVIEW TEXT {}:\\n{}\".format(i+1, reviews.Text[i]))\n",
    "    print(\"### SUMMARY {}:{}\".format(i+1, reviews.Summary[i]))\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text\n",
    "Let's clean up this text a bit to help our network -- we'll do the following:\n",
    "- Convert text to lowercase\n",
    "- Replace contractions with proper form\n",
    "- Remove unwanted characters\n",
    "- Remove stopwords (in the text but NOT the summary, to make summary sound natural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    uncontracted_text = []\n",
    "    \n",
    "    # Remove contractions\n",
    "    for word in text:\n",
    "        if word in text_cleaning.contractions:\n",
    "            uncontracted_text.append(text_cleaning.contractions[word])\n",
    "        else:\n",
    "            uncontracted_text.append(word)\n",
    "    text = \" \".join(uncontracted_text)\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = text.split()\n",
    "        text = [word for word in text if not word in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_summaries = [clean_text(text, remove_stopwords=False) for text in reviews.Summary]\n",
    "clean_texts     = [clean_text(text, remove_stopwords=True)  for text in reviews.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### REVIEW TEXT 1:\n",
      "bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "### SUMMARY 1:good quality dog food\n",
      "\n",
      "### REVIEW TEXT 2:\n",
      "product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "### SUMMARY 2:not as advertised\n",
      "\n",
      "### REVIEW TEXT 3:\n",
      "confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "### SUMMARY 3: delight  says it all\n",
      "\n",
      "### REVIEW TEXT 4:\n",
      "looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n",
      "### SUMMARY 4:cough medicine\n",
      "\n",
      "### REVIEW TEXT 5:\n",
      "great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "### SUMMARY 5:great taffy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_to_inspect = 5\n",
    "for i in range(reviews_to_inspect):\n",
    "    print(\"### REVIEW TEXT {}:\\n{}\".format(i+1, clean_texts[i]))\n",
    "    print(\"### SUMMARY {}:{}\".format(i+1, clean_summaries[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: saving cleaned texts / summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump the cleaned texts to save for later in case we need iti\n",
    "cleaned_texts_path = './checkpointed_data/cleaned_texts.p'\n",
    "pickle.dump((clean_texts, clean_summaries), open(cleaned_texts_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in cleaned data from checkpoint\n",
    "cleaned_texts_path = './checkpointed_data/cleaned_texts.p'\n",
    "clean_texts, clean_summaries = pickle.load(open(cleaned_texts_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the words into vectors\n",
    "\n",
    "We can't feed text directly into the model, and instead of one-hot encoding (which will make massive sparse matrices for each word in which most characters are 0), we'll instead use pre-trained word embeddings.\n",
    "\n",
    "<img src=\"images/word2vec_diagrams.png\"/>\n",
    "<i>source: https://deeplearning4j.org/word2vec.html</i>\n",
    "\n",
    "Instead of word2vec or GloVe, we'll use [ConceptNet Numberbatch](https://github.com/commonsense/conceptnet-numberbatch). This seems to be the best of everything since it has an ensemble of the above-mentioned word embeddings.\n",
    "\n",
    "<b>Formal attribution:<b>\n",
    "<i>This data contains semantic vectors from ConceptNet Numberbatch, by\n",
    "Luminoso Technologies, Inc. You may redistribute or modify the\n",
    "data under the terms of the CC-By-SA 4.0 license.</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's remove vocabulary words that are not in the ConceptNet (CN) embeddings; however, if these non-included words are showing up in reviews over a threshold (say 20), we'll still include them by assigning them a vector of random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_word_counts(clean_summaries, clean_texts):\n",
    "    total_counts = Counter()\n",
    "    for sentence in (clean_summaries + clean_texts):\n",
    "        for word in sentence.split():\n",
    "            if word not in total_counts:\n",
    "                total_counts[word] = 1\n",
    "            else:\n",
    "                total_counts[word] += 1\n",
    "    return total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of all vocabulary: 132884\n"
     ]
    }
   ],
   "source": [
    "word_counts = get_word_counts(clean_summaries, clean_texts)\n",
    "print(\"Total size of all vocabulary: {}\".format(len(word_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a word matrix from the ConceptNet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word embeddings from CN: 417195\n"
     ]
    }
   ],
   "source": [
    "embed_index = {}\n",
    "with open('./numberbatch-en-17.06.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embed_index[word] = embedding\n",
    "print(\"Total word embeddings from CN:\", len(embed_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find words that are more than our threshold but not in CN, so we can make our own embeddings for those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_missing_words(word_counts, embed_index):\n",
    "    not_in_cn = 0\n",
    "    word_threshold = 20 # If it appears more than 20 times, lets make our own embedding for it\n",
    "    missing_words = [word for word, count in word_counts.items() if (count > word_threshold and not word in embed_index)]\n",
    "    print(\"Words missing from CN: {}, ({}% of our vocabulary)\".format(len(missing_words), round(len(missing_words)/len(word_counts),4)*100))\n",
    "    return missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_dicts(word_counts, embed_index, threshold):\n",
    "    vocab_to_int = {}\n",
    "    value = 0\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= threshold or word in embed_index:\n",
    "            vocab_to_int[word] = value\n",
    "            value += 1\n",
    "    \n",
    "    # Special codes to include\n",
    "    codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]  \n",
    "    for code in codes:\n",
    "        vocab_to_int[code] = len(vocab_to_int)\n",
    "    \n",
    "    # Reverse dictionary\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "    \n",
    "    # Print stats\n",
    "    usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "    print(\"Total set of possible words:\", len(word_counts))\n",
    "    print(\"Number of words in our vocab:\", len(vocab_to_int))\n",
    "    print(\"Percent of words we're using: {}%\".format(usage_ratio))\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word_embed_matrix(vocab_to_int, embed_index, embedding_dim=300):\n",
    "    nb_words = len(vocab_to_int)\n",
    "    \n",
    "    # Create initial matrix of shape [nb_words,embedding_dim] with all zeros\n",
    "    word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    for word, idx in vocab_to_int.items():\n",
    "        if word in embed_index:\n",
    "            word_embedding_matrix[idx] = embed_index[word]\n",
    "        else:\n",
    "            # If it's not in CN, we make a random embedding\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            embed_index[word] = new_embedding\n",
    "            word_embedding_matrix[idx] = new_embedding\n",
    "    \n",
    "    print(\"Number of words in embedding matrix: \", len(word_embedding_matrix))\n",
    "    print(\"Number of words in vocab_to_int    : \", len(vocab_to_int))\n",
    "    return word_embedding_matrix\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total set of possible words: 132884\n",
      "Number of words in our vocab: 59595\n",
      "Percent of words we're using: 44.85%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab = word_dicts(word_counts, embed_index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in embedding matrix:  59595\n",
      "Number of words in vocab_to_int    :  59595\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = make_word_embed_matrix(vocab_to_int, embed_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually convert all words in both the text and clean summaries into their word-embedding representations.\n",
    "\n",
    "This means each input into our model (a review) is actually an array of size N where N = number of words (we'll use padding below so all reviews are the same length). So our total features will be size [M x N] where M is the number of reviews in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We're converting words to integers per vocab_to_int.\n",
    "We're also replacing words we don't know with UNK's code.\n",
    "And then adding an EOS token to end of each review.\n",
    "'''\n",
    "def convert_text_to_ints(text, vocab_to_int, word_count, unk_count, eos=False):\n",
    "    all_word_ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int['<UNK>'])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int['<EOS>'])\n",
    "        all_word_ints.append(sentence_ints)\n",
    "    return all_word_ints, word_count, unk_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in reviews (and summaries): 25679946\n",
      "Total number of UNKs in reviews (and summaries): 192245\n",
      "Percent of words that are UNK: 0.75%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "int_summaries, word_count, unk_count = convert_text_to_ints(clean_summaries, vocab_to_int, word_count, unk_count)\n",
    "\n",
    "# We are only adding <EOS> to the review (not the summary)\n",
    "int_texts, word_count, unk_count = convert_text_to_ints(clean_texts, vocab_to_int, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_perc = round(unk_count / word_count,4)*100\n",
    "print(\"Total number of words in reviews (and summaries):\", word_count)\n",
    "print(\"Total number of UNKs in reviews (and summaries):\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: saving vocab_to_int, int_to_vocab, word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump the data to save for later in case we need iti\n",
    "word_dicts = './checkpointed_data/word_dicts.p'\n",
    "pickle.dump((vocab_to_int, int_to_vocab, word_embedding_matrix), open(word_dicts, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in data from checkpoint\n",
    "word_dicts = './checkpointed_data/word_dicts.p'\n",
    "vocab_to_int, int_to_vocab, word_embedding_matrix = pickle.load(open(word_dicts, mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59595"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all sentences are replaced with the integer values for their respective words.\n",
    "\n",
    "Let's do the following to filter out the sentences we don't want to includes:\n",
    "- Only include reviews that are between a predefined min / max sentence length (we don't want super long ones or super short ones)\n",
    "- Remove reviews with too many UNK words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unk_counter(text):\n",
    "    unk_count = 0\n",
    "    for word in text:\n",
    "        if word == vocab_to_int['<UNK>']:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  568412.000000\n",
      "mean        4.181620\n",
      "std         2.657872\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         5.000000\n",
      "max        48.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  568412.000000\n",
      "mean       41.996782\n",
      "std        42.520854\n",
      "min         1.000000\n",
      "25%        18.000000\n",
      "50%        29.000000\n",
      "75%        50.000000\n",
      "max      2085.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n",
      "115.0\n",
      "207.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "9.0\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_final_data(int_summaries,\n",
    "                      int_texts, \n",
    "                      max_text_length, \n",
    "                      max_summary_length, \n",
    "                      unk_text_limit, \n",
    "                      unk_summary_limit):\n",
    "    \n",
    "    '''\n",
    "    Makes the final sorted summaries and sorted texts for our model to process\n",
    "    Params:\n",
    "        int_summaries      : summaries in word-int form\n",
    "        int_texts          : review texts in word-int form\n",
    "        max_text_length    : maximum allowed review text size\n",
    "        max_summary_length : maximum allowed summary size\n",
    "        unk_text_limit     : max number of UNKs allowed in review text\n",
    "        unk_summary_limit  : max number of UNKs allowed in summary\n",
    "    '''\n",
    "    \n",
    "    sorted_summaries = []\n",
    "    sorted_texts = []\n",
    "#     max_text_length = 84\n",
    "#     max_summary_length = 13\n",
    "    min_length = 2\n",
    "#     unk_text_limit = 1\n",
    "#     unk_summary_limit = 0\n",
    "\n",
    "    for length in range(min(lengths_texts.counts), max_text_length): \n",
    "        for count, words in enumerate(int_summaries):\n",
    "            if (len(int_summaries[count]) >= min_length and\n",
    "                len(int_summaries[count]) <= max_summary_length and\n",
    "                len(int_texts[count]) >= min_length and\n",
    "                unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "                unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "                length == len(int_texts[count])\n",
    "               ):\n",
    "                sorted_summaries.append(int_summaries[count])\n",
    "                sorted_texts.append(int_texts[count])\n",
    "\n",
    "    # Compare lengths to ensure they match\n",
    "    print(len(sorted_summaries))\n",
    "    print(len(sorted_texts))\n",
    "    return sorted_summaries, sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425616\n",
      "425616\n"
     ]
    }
   ],
   "source": [
    "sorted_summaries, sorted_texts = create_final_data(int_summaries,\n",
    "                                                   int_texts,\n",
    "                                                   84, 13, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: saving final data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump the data to save for later\n",
    "model_input_data_path = './checkpointed_data/model_input_data.p'\n",
    "pickle.dump((sorted_summaries, sorted_texts), open(model_input_data_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425616 425616\n"
     ]
    }
   ],
   "source": [
    "# Load in model input data from checkpoint\n",
    "model_input_data_path = './checkpointed_data/model_input_data.p'\n",
    "sorted_summaries, sorted_texts = pickle.load(open(model_input_data_path, mode='rb'))\n",
    "print(len(sorted_summaries), len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now we've gotten all our Amazon reviews and summaries in the proper form to go forward. We'll be doing further processing (adding PAD tokens, etc) when building the model itself.\n",
    "\n",
    "Our texts are now integer matrixes of length M x N (M = number of items, N = the word_integer in each item)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
