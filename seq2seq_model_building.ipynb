{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence Tensorflow model for Amazon reviews\n",
    "\n",
    "This notebook walks through training a [Sequence to sequence model](https://www.tensorflow.org/tutorials/seq2seq) with Tensorflow (version 1.1).\n",
    "\n",
    "The model is currently used as the predictive backend for the SUMZ chrome extension, which takes in Amazon reviews on the current web page and displays a small summary of each review. The model is trained on the the [Amazon fine food reviews dataset.](https://www.kaggle.com/snap/amazon-fine-food-reviews) from Kaggle, which consists of 568K review-summary pairs.\n",
    "\n",
    "This notebook goes through the following:\n",
    "- Building a sequence-to-sequence model using Tensorflow\n",
    "- Using the preprocessed data from the data_preprocessing notebook to train the model\n",
    "- Exporting the model into ProtoBuff format for serving in a production environment\n",
    "\n",
    "This builds on the [Text Summarization](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews) project by David Currie (this [Medium post](https://medium.com/towards-data-science/text-summarization-with-amazon-reviews-41801c2210b) goes into excellent detail as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use sequence to sequence RNN?\n",
    "\n",
    "Sequence-to-sequence models use two different RNNs, connected through the output state of the initial RNN. This is also called the encoder-decoder model (similar to Autoencoders).\n",
    "\n",
    "These seq2seq models are extremely powerful and versatile; they've been shown to have incredible performance a range of tasks including:\n",
    "\n",
    "| Task        | Input | Output\n",
    "|:------------- |:------------- | :--------\n",
    "| <b>Language translation</b>      | Text in language 1 | Text in language 2\n",
    "| <b>News headlines</b> | Text of news article | Short headline\n",
    "| <b>Question/Answering | Questions about content | Answers to questions\n",
    "| <b>Chatbots</b> | Incoming chat to bot | Reply from chatbot\n",
    "| <b>Smart email replies</b> | Email content | Reply to email\n",
    "| <b>Image captioning</b> |Image | Caption describing image\n",
    "| <b>Speech to text<b/> | Raw audio | Text of audio\n",
    "\n",
    "\n",
    "For more information, here are some great resources:\n",
    "- [Practical seq2seq](http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/)\n",
    "- [Tensorflow seq2seq tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials)\n",
    "- [Google talk by Quoc Le](https://www.youtube.com/watch?v=G5RY_SUJih4)\n",
    "- [Deep Learning for Chatbots](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)\n",
    "\n",
    "We're using it here to 'translate' from a sequence of words (the entirety of an Amazon review) and another sequence of words (the short summary of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "<img src=\"images/nct-seq2seq.png\"/>\n",
    "<i>seq2seq model. source: [WildML](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)</i>\n",
    "\n",
    "I'm building the model here piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inputs\n",
    "\n",
    "Our model has the following placeholders for input parameters:\n",
    "- <b>inputs</b>: The integer-transformed full review\n",
    "- <b>targets</b>: The integer-transformed summary of each review (with 'EOS' tags)\n",
    "- <b>learning_rate</b>: model training learning rate\n",
    "- <b>keep_probability</b>: For use in dropout\n",
    "- <b>target_seq_len</b>: Length of each summary we input into the model for training on\n",
    "- <b>max_target_seq_len</b>: Max length of the summaries we input into the model\n",
    "- <b>source_seq_len</b>: Length of reviews inputted into the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_input_placeholders():\n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None,None])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_probability = tf.placeholder(tf.float32, name='keep_probability')\n",
    "    target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "    max_target_seq_len = tf.reduce_max(target_seq_len, name='max_target_seq_len')\n",
    "    source_seq_len = tf.placeholder(tf.int32, (None,), name='source_seq_len')\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_probability, target_seq_len, max_target_seq_len, source_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "We are doing two things in this layer:\n",
    "1. Embedding the integer-value reviews into their word embeddings\n",
    "2. Feeding those embeddings into the encoder RNN network\n",
    "\n",
    "Here we're using the following Tensorflow APIs:\n",
    "* [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) convenient method to use a provided embedding layer and convert our integer input into embeddings for the encoder\n",
    "* [`tf.nn.bidirectional_dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn): The words in a review may depend heavily on sequences both before and after a given input, so we're using a [bidrectional RNN](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/bidirectional_rnn.py), consisting of:\n",
    "    * Multilayerd [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) \n",
    "    * The cell is wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bidirectional-rnn.png\"/>\n",
    "<center><i>Bidirectional RNN, source [wildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns)</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedded_encoder_input(input_data, word_embedding_matrix):\n",
    "    return tf.nn.embedding_lookup(word_embedding_matrix, input_data)\n",
    "\n",
    "def encoding_layer(encoder_inputs, rnn_size, source_seq_len, num_layers, keep_prob):\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            single_rnn_cell_forward = tf.contrib.rnn.LSTMCell(num_units = rnn_size,\n",
    "                                                      initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            single_rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(cell = single_rnn_cell_forward,\n",
    "                                                                    input_keep_prob = keep_prob)\n",
    "            single_rnn_cell_backward = tf.contrib.rnn.LSTMCell(num_units = rnn_size,\n",
    "                                                               initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            single_rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(cell = single_rnn_cell_backward,\n",
    "                                                                     input_keep_prob = keep_prob)\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(single_rnn_cell_forward,\n",
    "                                                                    single_rnn_cell_backward,\n",
    "                                                                    encoder_inputs,\n",
    "                                                                    source_seq_len,\n",
    "                                                                    dtype = tf.float32)\n",
    "    enc_output = tf.concat(enc_output, 2) # Concatenate both outputs together\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "We'll actually have 2 types of decoders, one for training and one for inference. In the training decoder, we'll feed in the next value in the target sequence <b>regardless of what the decoder outputs at each step</b>. We'll still use the decoder outputs in training though to calculate the loss for the model.\n",
    "\n",
    "This is called [Teacher Forcing](https://www.quora.com/What-is-the-teacher-forcing-in-RNN) since we're 'teaching' the decoder using actual target examples instead of letting it generate based on its own outputs at each time step. There are some known [issues](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/professor-forcing.md) with this regarding generation but we'll use it for now!\n",
    "\n",
    "This [notebook](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb) from Udacity does a great job at explaining how the two decoders work.\n",
    "\n",
    "<img src=\"images/udacity_seq2seqtraining.png\">\n",
    "<center><i>Image source: [Udacity](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)</i></center>\n",
    "<i>In the training decoder, we're using the output at each step to calculate the loss but <b>we're not using feeding that output into the next step</b>. Instead we feed it the next word in the actual target sequence regardless of what it outputs.</i>\n",
    "\n",
    "<img src=\"images/udacityseq2seqinference.png\">\n",
    "<center><i>Image source: [Udacity](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)</i></center>\n",
    "<i>In the inference decoder, we're feeding the output at each decoder step into the next decoder step as input.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the decoder input\n",
    "The first step is to process what the decoder gets as input. We do the following preprocessing steps:\n",
    "* We're removing the last element in each summary. Since the decoder doesn't need to see this last element (since the input to the last timestep is the second to last element in each target summary).\n",
    "* We're adding a <b>`<GO>`</b> token at the beginning of each summary so the decoder knows to start decoding at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "\n",
    "    # Remove the last word (integer) from each target sequence\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size,-1], [1,1])\n",
    "    \n",
    "    # Add the <GO> token to each target sequence\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedded_decoder_input(input_data, word_embedding_matrix):\n",
    "    return tf.nn.embedding_lookup(word_embedding_matrix, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the decoder cell\n",
    "\n",
    "The decoder cell is created very similarly to the encoder (though it's a single LSTM cell instead of a bidirectional RNN). This means we're using the [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) again and wrapping it in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper).\n",
    "\n",
    "<b>Attention Mechanism</b><br>\n",
    "\n",
    "A new feature we're adding here is an [attention mechanism](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/). Regular encoder-decoder seq2seq models use the final state of the encoder (the encoder's final output) as a sort of 'summary' of the entire input sentence -- in our case it means encoding the entire Amazon review into one fixed-length vector. Attention Mechanisms improve on this by allowing the decoder to 'focus' on different pieces of the input; it might weight certain words more heavily at different places in its output.\n",
    "\n",
    "<img src=\"images/attentionmech.png\">\n",
    "<center><i>Image source: [SPRO](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)</i></center>\n",
    "\n",
    "An example review might be:<br>\n",
    "\n",
    "<b><i>\"This product sucks. It wasn't what I was looking for and I'm upset I bought it!\"</i></b>\n",
    "\n",
    "For our summary, it'd be helpful to focus more on the word \"sucks\" in the first word of the summary; we'd hope to get a generated summary like <b><i>\"Terrible purchase\"</i></b>, with the word \"terrible\" heavily influenced by seeing \"sucks\" in the review.\n",
    "\n",
    "Compared to language translation and other sequence-to-sequence use cases, Attention Mechanisms seem especially helpful for our problem of summarization; a summary is by definition the distillation of information into a more compact form, meaning we are really trying to pay attention to the most important words in the original review.\n",
    "\n",
    "The Bhadanau attention style seems to give better results per this [paper](https://arxiv.org/abs/1703.03906v2).\n",
    "\n",
    "Here we're using the following Tensorflow APIs:\n",
    "* Multilayered [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) \n",
    "    * The cell is wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n",
    "* [`tf.contrib.seq2seq.BahdanauAttention`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention) as the attention mechanism\n",
    "    * Our LSTM cell wraps this using [`tf.contrib.seq2seq.DynamicAttentionWrapper`](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapper)\n",
    "    * We're setting the initial state of our attention wrapper with [`tf.contrib.seq2seq.DynamicAttentionWrapperState`](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapperState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_decoder_cell(rnn_size, \n",
    "                      num_layers, \n",
    "                      encoder_output, \n",
    "                      source_seq_len, \n",
    "                      keep_prob,\n",
    "                      batch_size,\n",
    "                      encoder_state):\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            single_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(single_cell, input_keep_prob=keep_prob)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                               encoder_output,\n",
    "                                                               source_seq_len,\n",
    "                                                               normalize=False,\n",
    "                                                               name='BahdanauAttention')\n",
    "    \n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                          attention_mechanism,\n",
    "                                                          rnn_size)\n",
    "\n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(encoder_state[0],\n",
    "                                                                    _zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32))\n",
    "    return dec_cell, initial_state \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the decoder layer\n",
    "\n",
    "Now we can build the decoding layer. Key to notice here is that we have two different decoders, one for training and one for inference (as detailed above).\n",
    "\n",
    "Both decoders share the same weights (hence the `reuse=True` tag in the variable scope). They have the 3 following pieces:\n",
    "* A Training Helper which reads in the inputs\n",
    "    * the training decoder uses the [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper) and is fed in the actual target summaries' words at each step\n",
    "    * the inference decoder uses the [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper) which takes the argmax of the logits at each step as the input to the next decoder step\n",
    "* Both decoders use [`tf.contrib.seq2seq.BasicDecoder`](tf.contrib.seq2seq.BasicDecoder) as their actual decoder\n",
    "* Both decoders use  [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode) to run the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(input_data,\n",
    "                   word_embedding_matrix,\n",
    "                   num_layers, \n",
    "                   rnn_size, \n",
    "                   keep_prob, \n",
    "                   encoder_output, \n",
    "                   source_seq_len,\n",
    "                   encoder_state,\n",
    "                   batch_size,\n",
    "                   vocab_size,\n",
    "                   target_seq_len,\n",
    "                   max_target_seq_len,\n",
    "                   vocab_to_int):\n",
    "    \n",
    "    decoder_embedded_input = embedded_decoder_input(input_data, word_embedding_matrix)\n",
    "    decoder_cell, initial_state = make_decoder_cell(rnn_size, \n",
    "                                                    num_layers, \n",
    "                                                    encoder_output, \n",
    "                                                    source_seq_len, \n",
    "                                                    keep_prob, \n",
    "                                                    batch_size,\n",
    "                                                    encoder_state)\n",
    "    output_layer = Dense(vocab_size,\n",
    "                        kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "    # Training\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embedded_input,\n",
    "                                                            sequence_length = target_seq_len,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer)\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                               output_time_major=False,\n",
    "                                                               impute_finished=True,\n",
    "                                                               maximum_iterations=max_target_seq_len)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True): # Reuse same params for inference\n",
    "        \n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), \n",
    "                               [batch_size], \n",
    "                               name='start_tokens')\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(word_embedding_matrix,\n",
    "                                                                    start_tokens,\n",
    "                                                                    vocab_to_int['<EOS>'])\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                              output_time_major=False,\n",
    "                                                              impute_finished=True,\n",
    "                                                              maximum_iterations=max_target_seq_len)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together - the full sequence-to-sequence model\n",
    "\n",
    "We piece together the <b>encoder</b>, the <b>decoder</b>, and return the training / inference logits from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_seq2seq(input_data, \n",
    "                 word_embedding_matrix,\n",
    "                 rnn_size,\n",
    "                 source_seq_len,\n",
    "                 num_layers,\n",
    "                 keep_prob,\n",
    "                 target_data,\n",
    "                 vocab_to_int,\n",
    "                 batch_size,\n",
    "                 vocab_size,\n",
    "                 target_seq_len,\n",
    "                 max_target_seq_len\n",
    "                 \n",
    "                 \n",
    "                 ):\n",
    "    \n",
    "    # Encoding layer\n",
    "    encoder_inputs = embedded_encoder_input(input_data, word_embedding_matrix)\n",
    "    encoder_output, encoder_state = encoding_layer(encoder_inputs, \n",
    "                                                   rnn_size, \n",
    "                                                   source_seq_len, \n",
    "                                                   num_layers, \n",
    "                                                   keep_prob)\n",
    "    \n",
    "    # Decoding layer\n",
    "    processed_decoder_input = process_decoder_input(target_data, \n",
    "                                                    vocab_to_int, \n",
    "                                                    batch_size)\n",
    "    training_logits, inference_logits = decoding_layer(processed_decoder_input,\n",
    "                                                       word_embedding_matrix,\n",
    "                                                       num_layers, \n",
    "                                                       rnn_size, \n",
    "                                                       keep_prob, \n",
    "                                                       encoder_output, \n",
    "                                                       source_seq_len,\n",
    "                                                       encoder_state,\n",
    "                                                       batch_size,\n",
    "                                                       vocab_size,\n",
    "                                                       target_seq_len,\n",
    "                                                       max_target_seq_len,\n",
    "                                                       vocab_to_int)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "In order to train, we need a `pad_batch` method that pads the batches so that each sequence has the same length, determined by the max item in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch_to_pad):\n",
    "    max_size = max([len(item) for item in batch_to_pad])\n",
    "    padded_batch = [item + [vocab_to_int['<PAD>']] * (max_size - len(item)) for item in batch_to_pad]\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, reviews, batch_size):\n",
    "    for batch_i in range(0, len(reviews)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        reviews_batch = reviews[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = pad_batch(summaries_batch)\n",
    "        pad_reviews_batch = pad_batch(reviews_batch)\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        pad_reviews_lengths = []\n",
    "        for review in pad_reviews_batch:\n",
    "            pad_reviews_lengths.append(len(review))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_reviews_batch, pad_summaries_lengths, pad_reviews_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "rnn_size = 256\n",
    "batch_size = 64\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_train_model(word_embedding_matrix, \n",
    "                rnn_size,\n",
    "                num_layers,\n",
    "                keep_probability,\n",
    "                vocab_to_int,\n",
    "                batch_size,\n",
    "                sorted_summaries,\n",
    "                sorted_reviews):\n",
    "    \n",
    "\n",
    "    # GRAPH BUILDING\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        \n",
    "        # Model inputs\n",
    "        inputs, targets, learning_rate, keep_probability, target_seq_len, max_target_seq_len, source_seq_len = model_input_placeholders()\n",
    "        \n",
    "        # Create final logits tensors\n",
    "        training_logits, inference_logits = full_seq2seq(tf.reverse(inputs, [-1]),\n",
    "                                                         word_embedding_matrix,\n",
    "                                                         rnn_size,\n",
    "                                                         source_seq_len,\n",
    "                                                         num_layers,\n",
    "                                                         keep_probability,\n",
    "                                                         targets,\n",
    "                                                         vocab_to_int,\n",
    "                                                         batch_size,\n",
    "                                                         len(vocab_to_int)+1,\n",
    "                                                         target_seq_len,\n",
    "                                                         max_target_seq_len)\n",
    "\n",
    "        training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "        inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        \n",
    "        masks = tf.sequence_mask(target_seq_len, max_target_seq_len, dtype=tf.float32, name='masks')\n",
    "        \n",
    "        # Set up optimizer\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            \n",
    "            cost = tf.contrib.seq2seq.sequence_loss(training_logits,\n",
    "                                                    targets,\n",
    "                                                    masks)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "            gradients = optimizer.compute_gradients(cost)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "            train_operation = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    print(\"Finished building the graph!\")\n",
    "    \n",
    "    start = 200000\n",
    "    end = start + 50000\n",
    "    sorted_summaries_short = sorted_summaries[start:end]\n",
    "    sorted_reviews_short = sorted_reviews[start:end]\n",
    "    print(\"The shortest review length:\", len(sorted_reviews_short[0]))\n",
    "    print(\"The longest review length:\", len(sorted_reviews_short[-1]))\n",
    "    \n",
    "#     learning_rate_decay = 0.95\n",
    "#     min_learning_rate = 0.0005\n",
    "    display_step = 20 # Check training loss after every 20 batches\n",
    "    stop = 5 # Stop training if average loss doesn't decrease in this mean update_checks\n",
    "    per_epoch = 3 # Make 3 update checks per epoch\n",
    "    update_check = (len(sorted_reviews_short)//batch_size//per_epoch)-1\n",
    "\n",
    "    update_loss = 0 \n",
    "    batch_loss = 0\n",
    "    summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "#     checkpoint = \"./model_checkpoints/best_model.ckpt\" \n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "            for batch_i, (summaries_batch, reviews_batch, summaries_lengths, reviews_lengths) in enumerate(\n",
    "                    get_batches(sorted_summaries_short, sorted_reviews_short, batch_size)):\n",
    "                start_time = time.time()\n",
    "                _, loss = sess.run(\n",
    "                    [train_operation, cost],\n",
    "                    {inputs: reviews_batch,\n",
    "                     targets: summaries_batch,\n",
    "                     learning_rate: lr,\n",
    "                     target_seq_len: summaries_lengths,\n",
    "                     source_seq_len: reviews_lengths,\n",
    "                     keep_probability: keep_prob})\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(sorted_reviews_short) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "#             learning_rate *= learning_rate_decay\n",
    "#             if learning_rate < min_learning_rate:\n",
    "#                 learning_rate = min_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickled_data():\n",
    "    word_dicts_path = './checkpointed_data/word_dicts.p'\n",
    "    model_input_data_path = './checkpointed_data/model_input_data.p'\n",
    "    vocab_to_int, int_to_vocab, word_embedding_matrix = pickle.load(open(word_dicts_path, mode='rb'))\n",
    "    sorted_summaries, sorted_reviews = pickle.load(open(model_input_data_path, mode='rb'))\n",
    "    return vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews = load_pickled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building the graph!\n",
      "The shortest review length: 25\n",
      "The longest review length: 31\n",
      "Epoch   1/100 Batch   20/781 - Loss:  4.377, Seconds: 58.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9e2a272f1fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0msorted_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                       sorted_reviews)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-bbb4797722e8>\u001b[0m in \u001b[0;36mbuild_and_train_model\u001b[0;34m(word_embedding_matrix, rnn_size, num_layers, keep_probability, vocab_to_int, batch_size, sorted_summaries, sorted_reviews)\u001b[0m\n\u001b[1;32m     85\u001b[0m                      \u001b[0mtarget_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                      \u001b[0msource_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreviews_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                      keep_probability: keep_prob})\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "build_and_train_model(word_embedding_matrix, \n",
    "                      rnn_size,\n",
    "                      num_layers,\n",
    "                      keep_prob,\n",
    "                      vocab_to_int,\n",
    "                      batch_size,\n",
    "                      sorted_summaries,\n",
    "                      sorted_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model results\n",
    "Note -- I was training this on my Macbook Pro but switched to an AWS EC2 instance with a GPU to train faster. The final results I got are below (this is the model being used in the summary generation below)\n",
    "\n",
    "<img src=\"images/ec2loss.png\"><br>\n",
    "<center><i>Results after training on GPU</i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing out summary generation\n",
    "Here's a function that takes in an input review and gives us back the generated summary from the model, so we can see what kind of summaries it comes out with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import text_cleaning\n",
    "\n",
    "def create_summary(input_review):\n",
    "    \n",
    "    # Clean the text to get it ready for the model\n",
    "    text = text_cleaning.clean_text(input_review)\n",
    "    text_int = [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n",
    "    \n",
    "    # Load in the model from the checkpoint\n",
    "    checkpoint = \"./model_checkpoints/best_model.ckpt\"\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "        loader.restore(sess, checkpoint)\n",
    "        \n",
    "        # The input tensors for inference\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        inference_logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        source_seq_len = loaded_graph.get_tensor_by_name('source_seq_len:0')\n",
    "        target_seq_len = loaded_graph.get_tensor_by_name('target_seq_len:0')\n",
    "        keep_probability = loaded_graph.get_tensor_by_name('keep_probability:0')\n",
    "        \n",
    "        # Run the graph to get the summary logits\n",
    "        summary_logits = sess.run(inference_logits, { input_data: [text_int] * batch_size,\n",
    "                                                      target_seq_len: [np.random.randint(5,8)],\n",
    "                                                      source_seq_len: [len(text_int)] * batch_size,\n",
    "                                                      keep_probability: 1.0\n",
    "                                                    })\n",
    "        # This returns a batch_size - length matrix of logits, so we just need the first one\n",
    "        summary_logits = summary_logits[0]\n",
    "        \n",
    "        # Convert back to human text to print out\n",
    "        pad = vocab_to_int[\"<PAD>\"]\n",
    "        \n",
    "        return \" \".join([int_to_vocab[i] for i in summary_logits if i != pad])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can try it out on sample reviews (note: in practical cases we'd send in all the reviews as a batch instead of iterating through each one). They're not perfect, but quite neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "['best cereal ever', 'best chocolate ever', 'best jerky ever']\n",
      "Took 28.24122966499999 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "sample_reviews = [\"It's become my favorite oatmeal, I even eat it for dinner from time to time...\",\n",
    "                  \"The chocolate was splendid, tasty. Highly recommended\",\n",
    "                  \"Absolutely the worst cheese I've had. Completely rotten, don't buy this!\"]\n",
    "\n",
    "summaries = [create_summary(review) for review in sample_reviews]\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print([summary for summary in summaries])\n",
    "print(\"Took {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model for serving\n",
    "Since we're going to use this model for inference in a production app (the Sumz chrome extension), we need to export it. Here's a great [tutorial](https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc) on how to go about this, which I'm following below.\n",
    "\n",
    "The gist is that we want to export just what we need for inference -- this significantly pares down the bloat in the model so we can serve it easier in production. We really just want the model and its weights in one file; essentially we want to freeze the model.\n",
    "\n",
    "The <b>checkpoints</b> we were saving above while training look like this:\n",
    "<img src=\"images/checkpoints_files.png\"><br>\n",
    "These are the following:\n",
    "* <b>best_model.ckpt.meta</b> This is holding the graph and metadata\n",
    "* <b>best_model.ckpt.index</b> Immutable key-value table that links each serialized tensor name and where to find it in the .data files\n",
    "* <b>best_model.ckpt.data-00000-of-00001</b> Holds the weights of the model\n",
    "* <b>checkpoint</b> High-level helper for loading different checkpoint files\n",
    "\n",
    "For production purposes we can 'freeze' the meta-graph, restore the weights. We'll also only need whatever is necessary for inference, which the following functino helps us do.\n",
    "\n",
    "The serialized frozen model is saved as a [ProtoBuf](https://developers.google.com/protocol-buffers/) file (a super compressed way to save the model and reload it); this is the final frozen model we'll use to serve to real users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "INFO:tensorflow:Froze 15 variables.\n",
      "Converted 15 variables to const ops.\n",
      "529 ops in final graph\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = './models/frozen_seq2seq_model.pb'\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    loader = tf.train.import_meta_graph('./model_checkpoints/best_model.ckpt.meta')\n",
    "    loader.restore(sess, './model_checkpoints/best_model.ckpt')\n",
    "\n",
    "    # Print out the operation names if needed\n",
    "    # ops = sess.graph.get_operations()\n",
    "#     for op in ops:\n",
    "#         print(op.name)\n",
    "\n",
    "    # We want the just the predictions operation (and all necessary variables for that)\n",
    "    # This converts those variables to constants (freezing them)\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n",
    "                                                                    tf.get_default_graph().as_graph_def(),\n",
    "                                                                    output_node_names=[\"predictions\"])\n",
    "    output_graph = MODEL_PATH\n",
    "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "    print(\"%d ops in final graph\" % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-loading the frozen model in\n",
    "Now let's try reloading the model in and using it for inference on the same reviews above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_graph():\n",
    "    with tf.gfile.GFile(MODEL_PATH, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph\n",
    "\n",
    "def summary_from_frozen_model(input_review):\n",
    "\n",
    "    text = text_cleaning.clean_text(input_review)\n",
    "    text_int = [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n",
    "    graph = load_graph()\n",
    "\n",
    "#     for op in graph.get_operations():\n",
    "#         print(op.name)\n",
    "\n",
    "    batch_size = 64 # Match original batch size    \n",
    "    input_data = graph.get_tensor_by_name('import/input:0')\n",
    "    target_seq_len = graph.get_tensor_by_name('import/target_seq_len:0')\n",
    "    source_seq_len = graph.get_tensor_by_name('import/source_seq_len:0')\n",
    "    keep_probability = graph.get_tensor_by_name('import/keep_probability:0')\n",
    "    inference_logits = graph.get_tensor_by_name('import/predictions:0')\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        feed = {\n",
    "            input_data: [text_int]*batch_size, \n",
    "            target_seq_len: [np.random.randint(5,8)], \n",
    "            source_seq_len: [len(text_int)]*batch_size,\n",
    "            keep_probability: 1.0}\n",
    "        summary_logits = sess.run(inference_logits, feed_dict = feed)[0]\n",
    "    \n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "    return \" \".join([int_to_vocab[i] for i in summary_logits if i != pad])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best cereal ever', 'best chocolate ever', 'best jerky ever']\n",
      "Took 11.593751017935574 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "sample_reviews = [\"It's become my favorite oatmeal, I even eat it for dinner from time to time...\",\n",
    "                  \"The chocolate was splendid, tasty. Highly recommended\",\n",
    "                  \"Absolutely the worst cheese I've had. Completely rotten, don't buy this!\"]\n",
    "\n",
    "summaries = [summary_from_frozen_model(review) for review in sample_reviews]\n",
    "\n",
    "print([summary for summary in summaries])\n",
    "\n",
    "end = timer()\n",
    "print(\"Took {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device to node 'MatMul_10': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n\t [[Node: MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](Placeholder_1, Placeholder_1)]]\n\nCaused by op 'MatMul_10', defined at:\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-e5c713ce2a43>\", line 55, in <module>\n    c1.append(matpow(b, n))\n  File \"<ipython-input-16-e5c713ce2a43>\", line 45, in matpow\n    return tf.matmul(M, matpow(M, n-1))\n  File \"<ipython-input-16-e5c713ce2a43>\", line 45, in matpow\n    return tf.matmul(M, matpow(M, n-1))\n  File \"<ipython-input-16-e5c713ce2a43>\", line 45, in matpow\n    return tf.matmul(M, matpow(M, n-1))\n  [Previous line repeated 6 more times]\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1801, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1263, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul_10': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n\t [[Node: MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](Placeholder_1, Placeholder_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1066\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device to node 'MatMul_10': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n\t [[Node: MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](Placeholder_1, Placeholder_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e5c713ce2a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Run the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mt2_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device to node 'MatMul_10': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n\t [[Node: MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](Placeholder_1, Placeholder_1)]]\n\nCaused by op 'MatMul_10', defined at:\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-e5c713ce2a43>\", line 55, in <module>\n    c1.append(matpow(b, n))\n  File \"<ipython-input-16-e5c713ce2a43>\", line 45, in matpow\n    return tf.matmul(M, matpow(M, n-1))\n  File \"<ipython-input-16-e5c713ce2a43>\", line 45, in matpow\n    return tf.matmul(M, matpow(M, n-1))\n  File \"<ipython-input-16-e5c713ce2a43>\", line 45, in matpow\n    return tf.matmul(M, matpow(M, n-1))\n  [Previous line repeated 6 more times]\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1801, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1263, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ashnkumar/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul_10': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n\t [[Node: MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](Placeholder_1, Placeholder_1)]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "'''\n",
    "Basic Multi GPU computation example using TensorFlow library.\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "'''\n",
    "This tutorial requires your machine to have 2 GPUs\n",
    "\"/cpu:0\": The CPU of your machine.\n",
    "\"/gpu:0\": The first GPU of your machine\n",
    "\"/gpu:1\": The second GPU of your machine\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "# Processing Units logs\n",
    "log_device_placement = True\n",
    "\n",
    "# Num of multiplications to perform\n",
    "n = 10\n",
    "\n",
    "'''\n",
    "Example: compute A^n + B^n on 2 GPUs\n",
    "Results on 8 cores with 2 GTX-980:\n",
    " * Single GPU computation time: 0:00:11.277449\n",
    " * Multi GPU computation time: 0:00:07.131701\n",
    "'''\n",
    "# Create random large matrix\n",
    "A = np.random.rand(10000, 10000).astype('float32')\n",
    "B = np.random.rand(10000, 10000).astype('float32')\n",
    "\n",
    "# Create a graph to store results\n",
    "c1 = []\n",
    "c2 = []\n",
    "\n",
    "def matpow(M, n):\n",
    "    if n < 1: #Abstract cases where n < 1\n",
    "        return M\n",
    "    else:\n",
    "        return tf.matmul(M, matpow(M, n-1))\n",
    "\n",
    "'''\n",
    "Single GPU computing\n",
    "'''\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.placeholder(tf.float32, [10000, 10000])\n",
    "    b = tf.placeholder(tf.float32, [10000, 10000])\n",
    "    # Compute A^n and B^n and store results in c1\n",
    "    c1.append(matpow(a, n))\n",
    "    c1.append(matpow(b, n))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c1) #Addition of all elements in c1, i.e. A^n + B^n\n",
    "\n",
    "t1_1 = datetime.datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
    "    # Run the op.\n",
    "    sess.run(sum, {a:A, b:B})\n",
    "t2_1 = datetime.datetime.now()\n",
    "\n",
    "\n",
    "'''\n",
    "Multi GPU computing\n",
    "'''\n",
    "# GPU:0 computes A^n\n",
    "with tf.device('/gpu:0'):\n",
    "    # Compute A^n and store result in c2\n",
    "    a = tf.placeholder(tf.float32, [10000, 10000])\n",
    "    c2.append(matpow(a, n))\n",
    "\n",
    "# GPU:1 computes B^n\n",
    "with tf.device('/gpu:1'):\n",
    "    # Compute B^n and store result in c2\n",
    "    b = tf.placeholder(tf.float32, [10000, 10000])\n",
    "    c2.append(matpow(b, n))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n",
    "\n",
    "t1_2 = datetime.datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
    "    # Run the op.\n",
    "    sess.run(sum, {a:A, b:B})\n",
    "t2_2 = datetime.datetime.now()\n",
    "\n",
    "\n",
    "print(\"Single GPU computation time: \" + str(t2_1-t1_1))\n",
    "print(\"Multi GPU computation time: \" + str(t2_2-t1_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You can find the model working through the Sumz chrome extension (link); clearly it's not perfect at its summaries but it's cool that it can have some utility.\n",
    "\n",
    "Importantly I got to cover some of the concepts I wanted to learn more about, including:\n",
    "* Sequence to sequence models\n",
    "* Attention Mechanisms\n",
    "* Serving the model in a production environment (exporting)\n",
    "\n",
    "There are definitely some improvements to be made:\n",
    "* <b>Better data</b>: this model uses a fine foods dataset, and only a small portion of it. It'd be great to train it was a much bigger Amazon reviews set like [SNAP](https://snap.stanford.edu/data/web-Amazon.html) (34M reviews)\n",
    "* <b>Fine tuning on the model</b>: Especially the Attention Mechanism, and other architectures\n",
    "* <b>Bucketing</b>The review lengths vary wildly for Amazon reviews; having buckets could really help (putting reviews within a certain length in one bucket) so that the model can be trained based on that. Also having more types of products in the reviews could help inform the model in terms of summaries\n",
    "\n",
    "Hope this was helpful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
