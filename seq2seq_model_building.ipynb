{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence Tensorflow model for Amazon reviews\n",
    "\n",
    "This notebook walks through training a [Sequence to sequence model](https://www.tensorflow.org/tutorials/seq2seq) with Tensorflow (version 1.1).\n",
    "\n",
    "The model is currently used as the predictive backend for the SUMZ chrome extension, which takes in Amazon reviews on the current web page and displays a small summary of each review. The model is trained on the the [Amazon fine food reviews dataset.](https://www.kaggle.com/snap/amazon-fine-food-reviews) from Kaggle, which consists of 568K review-summary pairs.\n",
    "\n",
    "This notebook goes through the following:\n",
    "- Building a sequence-to-sequence model using Tensorflow\n",
    "- Using the preprocessed data from this notebook ASDF to train the model\n",
    "- Exporting the model into ProtoBuff format for serving in a production environment\n",
    "\n",
    "This builds on the [Text Summarization](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews) project by David Currie (this [Medium post](https://medium.com/towards-data-science/text-summarization-with-amazon-reviews-41801c2210b) goes into excellent detail as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use sequence to sequence RNN?\n",
    "\n",
    "Sequence-to-sequence models use two different RNNs, connected through the output state of the initial RNN. This is also called the encoder-decoder model (similar to Autoencoders).\n",
    "\n",
    "These seq2seq models are extremely powerful and versatile; they've been shown to have incredible performance a range of tasks including:\n",
    "\n",
    "| Task        | Input | Output\n",
    "|:------------- |:------------- | :--------\n",
    "| <b>Language translation</b>      | Text in language 1 | Text in language 2\n",
    "| <b>News headlines</b> | Text of news article | Short headline\n",
    "| <b>Question/Answering | Questions about content | Answers to questions\n",
    "| <b>Chatbots</b> | Incoming chat to bot | Reply from chatbot\n",
    "| <b>Smart email replies</b> | Email content | Reply to email\n",
    "| <b>Image captioning</b> |Image | Caption describing image\n",
    "| <b>Speech to text<b/> | Raw audio | Text of audio\n",
    "\n",
    "\n",
    "For more information, here are some great resources:\n",
    "- [Practical seq2seq](http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/)\n",
    "- [Tensorflow seq2seq tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials)\n",
    "- [Google talk by Quoc Le](https://www.youtube.com/watch?v=G5RY_SUJih4)\n",
    "- [Deep Learning for Chatbots](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)\n",
    "\n",
    "We're using it here to 'translate' from a sequence of words (the entirety of an Amazon review) and another sequence of words (the short summary of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "<img src=\"images/nct-seq2seq.png\"/>\n",
    "<center><i>seq2seq model. source: [WildML](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)</i></center>\n",
    "\n",
    "I'm building the model here piece by piece, and at the end combine it into its own class. (You can find the final model class in its own file here [todo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inputs\n",
    "\n",
    "Our model has the following placeholders for input parameters:\n",
    "- <b>inputs</b>: The integer-transformed full review\n",
    "- <b>targets</b>: The integer-transformed summary of each review (with 'EOS' tags)\n",
    "- <b>learning_rate</b>: model training learning rate\n",
    "- <b>keep_probability</b>: For use in dropout\n",
    "- <b>target_seq_len</b>: Length of each summary we input into the model for training on\n",
    "- <b>max_target_seq_len</b>: Max length of the summaries we input into the model\n",
    "- <b>source_seq_len</b>: Length of reviews inputted into the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_input_placeholders():\n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None,None])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_probability = tf.placeholder(tf.float32, name='keep_probability')\n",
    "    target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "    max_target_seq_len = tf.reduce_max(target_seq_len, name='max_target_seq_len')\n",
    "    source_seq_len = tf.placeholder(tf.int32, (None,), name='source_seq_len')\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_probability, target_seq_len, max_target_seq_len, source_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "We are doing two things in this layer:\n",
    "1. Embedding the integer-value reviews into their word embeddings\n",
    "2. Feeding those embeddings into the encoder RNN network\n",
    "\n",
    "Here we're using the following Tensorflow APIs:\n",
    "* [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) convenient method to use a provided embedding layer and convert our integer input into embeddings for the encoder\n",
    "* [`tf.nn.bidirectional_dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn): The words in a review may depend heavily on sequences both before and after a given input, so we're using a [bidrectional RNN](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/bidirectional_rnn.py), consisting of:\n",
    "    * Multilayerd [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) \n",
    "    * The cell is wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bidirectional-rnn.png\"/>\n",
    "<center><i>Bidirectional RNN, source [wildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns)</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedded_encoder_input(input_data, word_embedding_matrix):\n",
    "    return tf.nn.embedding_lookup(word_embedding_matrix, input_data)\n",
    "\n",
    "def encoding_layer(encoder_inputs, rnn_size, source_seq_len, num_layers, keep_prob):\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            single_rnn_cell_forward = tf.contrib.rnn.LSTMCell(num_units = rnn_size,\n",
    "                                                      initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            single_rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(cell = single_rnn_cell_forward,\n",
    "                                                                    input_keep_prob = keep_prob)\n",
    "            single_rnn_cell_backward = tf.contrib.rnn.LSTMCell(num_units = rnn_size,\n",
    "                                                               initializer = tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            single_rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(cell = single_rnn_cell_backward,\n",
    "                                                                     input_keep_prob = keep_prob)\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(single_rnn_cell_forward,\n",
    "                                                                    single_rnn_cell_backward,\n",
    "                                                                    encoder_inputs,\n",
    "                                                                    source_seq_len,\n",
    "                                                                    dtype = tf.float32)\n",
    "    enc_output = tf.concat(enc_output, 2) # Concatenate both outputs together\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "We'll actually have 2 types of decoders, one for training and one for inference. In the training decoder, we'll feed in the next value in the target sequence <b>regardless of what the decoder outputs at each step</b>. We'll still use the decoder outputs in training though to calculate the loss for the model.\n",
    "\n",
    "This is called [Teacher Forcing](https://www.quora.com/What-is-the-teacher-forcing-in-RNN) since we're 'teaching' the decoder using actual target examples instead of letting it generate based on its own outputs at each time step. There are some known [issues](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/professor-forcing.md) with this regarding generation but we'll use it for now!\n",
    "\n",
    "This [notebook](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb) from Udacity does a great job at explaining how the two decoders work.\n",
    "\n",
    "<img src=\"images/udacity_seq2seqtraining.png\">\n",
    "<center><i>Image source: [Udacity](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)</i></center>\n",
    "<i>In the training decoder, we're using the output at each step to calculate the loss but <b>we're not using feeding that output into the next step</b>. Instead we feed it the next word in the actual target sequence regardless of what it outputs.</i>\n",
    "\n",
    "<img src=\"images/udacityseq2seqinference.png\">\n",
    "<center><i>Image source: [Udacity](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)</i></center>\n",
    "<i>In the inference decoder, we're feeding the output at each decoder step into the next decoder step as input.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the decoder input\n",
    "The first step is to process what the decoder gets as input. We do the following preprocessing steps:\n",
    "* We're removing the last element in each summary. Since the decoder doesn't need to see this last element (since the input to the last timestep is the second to last element in each target summary).\n",
    "* We're adding a <b>`<GO>`</b> token at the beginning of each summary so the decoder knows to start decoding at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "\n",
    "    # Remove the last word (integer) from each target sequence\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size,-1], [1,1])\n",
    "    \n",
    "    # Add the <GO> token to each target sequence\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedded_decoder_input(input_data, word_embedding_matrix):\n",
    "    return tf.nn.embedding_lookup(word_embedding_matrix, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the decoder cell\n",
    "\n",
    "The decoder cell is created very similarly to the encoder (though it's a single LSTM cell instead of a bidirectional RNN). This means we're using the [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) again and wrapping it in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper).\n",
    "\n",
    "<b>Attention Mechanism</b><br>\n",
    "\n",
    "A new feature we're adding here is an [attention mechanism](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/). Regular encoder-decoder seq2seq models use the final state of the encoder (the encoder's final output) as a sort of 'summary' of the entire input sentence -- in our case it means encoding the entire Amazon review into one fixed-length vector. Attention Mechanisms improve on this by allowing the decoder to 'focus' on different pieces of the input; it might weight certain words more heavily at different places in its output.\n",
    "\n",
    "<img src=\"images/attentionmech.png\">\n",
    "<center><i>Image source: [SPRO](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)</i></center>\n",
    "\n",
    "An example review might be:<br>\n",
    "\n",
    "<b><i>\"This product sucks. It wasn't what I was looking for and I'm upset I bought it!\"</i></b>\n",
    "\n",
    "For our summary, it'd be helpful to focus more on the word \"sucks\" in the first word of the summary; we'd hope to get a generated summary like <b><i>\"Terrible purchase\"</i></b>, with the word \"terrible\" heavily influenced by seeing \"sucks\" in the review.\n",
    "\n",
    "Compared to language translation and other sequence-to-sequence use cases, Attention Mechanisms seem especially helpful for our problem of summarization; a summary is by definition the distillation of information into a more compact form, meaning we are really trying to pay attention to the most important words in the original review.\n",
    "\n",
    "The Bhadanau attention style seems to give better results per this [paper](https://arxiv.org/abs/1703.03906v2).\n",
    "\n",
    "Here we're using the following Tensorflow APIs:\n",
    "* Multilayered [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) \n",
    "    * The cell is wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n",
    "* [`tf.contrib.seq2seq.BahdanauAttention`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention) as the attention mechanism\n",
    "    * Our LSTM cell wraps this using [`tf.contrib.seq2seq.DynamicAttentionWrapper`](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapper)\n",
    "    * We're setting the initial state of our attention wrapper with [`tf.contrib.seq2seq.DynamicAttentionWrapperState`](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapperState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_decoder_cell(rnn_size, \n",
    "                      num_layers, \n",
    "                      encoder_output, \n",
    "                      source_seq_len, \n",
    "                      keep_prob,\n",
    "                      batch_size,\n",
    "                      encoder_state):\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            single_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(single_cell, input_keep_prob=keep_prob)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                               encoder_output,\n",
    "                                                               source_seq_len,\n",
    "                                                               normalize=False,\n",
    "                                                               name='BahdanauAttention')\n",
    "    \n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                          attention_mechanism,\n",
    "                                                          rnn_size)\n",
    "\n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(encoder_state[0],\n",
    "                                                                    _zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32))\n",
    "    return dec_cell, initial_state \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the decoder layer\n",
    "\n",
    "Now we can build the decoding layer. Key to notice here is that we have two different decoders, one for training and one for inference (as detailed above).\n",
    "\n",
    "Both decoders share the same weights (hence the `reuse=True` tag in the variable scope). They have the 3 following pieces:\n",
    "* A Training Helper which reads in the inputs\n",
    "    * the training decoder uses the [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper) and is fed in the actual target summaries' words at each step\n",
    "    * the inference decoder uses the [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper) which takes the argmax of the logits at each step as the input to the next decoder step\n",
    "* Both decoders use [`tf.contrib.seq2seq.BasicDecoder`](tf.contrib.seq2seq.BasicDecoder) as their actual decoder\n",
    "* Both decoders use  [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode) to run the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(input_data,\n",
    "                   word_embedding_matrix,\n",
    "                   num_layers, \n",
    "                   rnn_size, \n",
    "                   keep_prob, \n",
    "                   encoder_output, \n",
    "                   source_seq_len,\n",
    "                   encoder_state,\n",
    "                   batch_size,\n",
    "                   vocab_size,\n",
    "                   target_seq_len,\n",
    "                   max_target_seq_len,\n",
    "                   vocab_to_int):\n",
    "    \n",
    "    decoder_embedded_input = embedded_decoder_input(input_data, word_embedding_matrix)\n",
    "    decoder_cell, initial_state = make_decoder_cell(rnn_size, \n",
    "                                                    num_layers, \n",
    "                                                    encoder_output, \n",
    "                                                    source_seq_len, \n",
    "                                                    keep_prob, \n",
    "                                                    batch_size,\n",
    "                                                    encoder_state)\n",
    "    output_layer = Dense(vocab_size,\n",
    "                        kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "    # Training\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embedded_input,\n",
    "                                                            sequence_length = target_seq_len,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer)\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                               output_time_major=False,\n",
    "                                                               impute_finished=True,\n",
    "                                                               maximum_iterations=max_target_seq_len)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True): # Reuse same params for inference\n",
    "        \n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), \n",
    "                               [batch_size], \n",
    "                               name='start_tokens')\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(word_embedding_matrix,\n",
    "                                                                    start_tokens,\n",
    "                                                                    vocab_to_int['<EOS>'])\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                              output_time_major=False,\n",
    "                                                              impute_finished=True,\n",
    "                                                              maximum_iterations=max_target_seq_len)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together - the full sequence-to-sequence model\n",
    "\n",
    "We piece together the <b>encoder</b>, the <b>decoder</b>, and return the training / inference logits from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_seq2seq(input_data, \n",
    "                 word_embedding_matrix,\n",
    "                 rnn_size,\n",
    "                 source_seq_len,\n",
    "                 num_layers,\n",
    "                 keep_prob,\n",
    "                 target_data,\n",
    "                 vocab_to_int,\n",
    "                 batch_size,\n",
    "                 vocab_size,\n",
    "                 target_seq_len,\n",
    "                 max_target_seq_len\n",
    "                 \n",
    "                 \n",
    "                 ):\n",
    "    \n",
    "    # Encoding layer\n",
    "    encoder_inputs = embedded_encoder_input(input_data, word_embedding_matrix)\n",
    "    encoder_output, encoder_state = encoding_layer(encoder_inputs, \n",
    "                                                   rnn_size, \n",
    "                                                   source_seq_len, \n",
    "                                                   num_layers, \n",
    "                                                   keep_prob)\n",
    "    \n",
    "    # Decoding layer\n",
    "    processed_decoder_input = process_decoder_input(target_data, \n",
    "                                                    vocab_to_int, \n",
    "                                                    batch_size)\n",
    "    training_logits, inference_logits = decoding_layer(processed_decoder_input,\n",
    "                                                       word_embedding_matrix,\n",
    "                                                       num_layers, \n",
    "                                                       rnn_size, \n",
    "                                                       keep_prob, \n",
    "                                                       encoder_output, \n",
    "                                                       source_seq_len,\n",
    "                                                       encoder_state,\n",
    "                                                       batch_size,\n",
    "                                                       vocab_size,\n",
    "                                                       target_seq_len,\n",
    "                                                       max_target_seq_len,\n",
    "                                                       vocab_to_int)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "In order to train, we need a `pad_batch` method that pads the batches so that each sequence has the same length, determined by the max item in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch_to_pad):\n",
    "    max_size = max([len(item) for item in batch_to_pad])\n",
    "    padded_batch = [item + [vocab_to_int['<PAD>']] * (max_size - len(item)) for item in batch_to_pad]\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, reviews, batch_size):\n",
    "    for batch_i in range(0, len(reviews)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        reviews_batch = reviews[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = pad_batch(summaries_batch)\n",
    "        pad_reviews_batch = pad_batch(reviews_batch)\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        pad_reviews_lengths = []\n",
    "        for review in pad_reviews_batch:\n",
    "            pad_reviews_lengths.append(len(review))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_reviews_batch, pad_summaries_lengths, pad_reviews_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "rnn_size = 256\n",
    "batch_size = 64\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_train_model(word_embedding_matrix, \n",
    "                rnn_size,\n",
    "                num_layers,\n",
    "                keep_probability,\n",
    "                vocab_to_int,\n",
    "                batch_size,\n",
    "                sorted_summaries,\n",
    "                sorted_reviews):\n",
    "    \n",
    "\n",
    "    # GRAPH BUILDING\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        \n",
    "        # Model inputs\n",
    "        inputs, targets, learning_rate, keep_probability, target_seq_len, max_target_seq_len, source_seq_len = model_input_placeholders()\n",
    "        \n",
    "        # Create final logits tensors\n",
    "        training_logits, inference_logits = full_seq2seq(tf.reverse(inputs, [-1]),\n",
    "                                                         word_embedding_matrix,\n",
    "                                                         rnn_size,\n",
    "                                                         source_seq_len,\n",
    "                                                         num_layers,\n",
    "                                                         keep_probability,\n",
    "                                                         targets,\n",
    "                                                         vocab_to_int,\n",
    "                                                         batch_size,\n",
    "                                                         len(vocab_to_int)+1,\n",
    "                                                         target_seq_len,\n",
    "                                                         max_target_seq_len)\n",
    "\n",
    "        training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "        inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        \n",
    "        masks = tf.sequence_mask(target_seq_len, max_target_seq_len, dtype=tf.float32, name='masks')\n",
    "        \n",
    "        # Set up optimizer\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            \n",
    "            cost = tf.contrib.seq2seq.sequence_loss(training_logits,\n",
    "                                                    targets,\n",
    "                                                    masks)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "            gradients = optimizer.compute_gradients(cost)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "            train_operation = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    print(\"Finished building the graph!\")\n",
    "    \n",
    "    start = 200000\n",
    "    end = start + 50000\n",
    "    sorted_summaries_short = sorted_summaries[start:end]\n",
    "    sorted_reviews_short = sorted_reviews[start:end]\n",
    "    print(\"The shortest review length:\", len(sorted_reviews_short[0]))\n",
    "    print(\"The longest review length:\", len(sorted_reviews_short[-1]))\n",
    "    \n",
    "#     learning_rate_decay = 0.95\n",
    "#     min_learning_rate = 0.0005\n",
    "    display_step = 10 # Check training loss after every 20 batches\n",
    "    stop = 5 # Stop training if average loss doesn't decrease in this mean update_checks\n",
    "    per_epoch = 3 # Make 3 update checks per epoch\n",
    "    update_check = (len(sorted_reviews_short)//batch_size//per_epoch)-1\n",
    "\n",
    "    update_loss = 0 \n",
    "    batch_loss = 0\n",
    "    summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "    checkpoint = \"./model_checkpoints/best_model.ckpt\" \n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "            for batch_i, (summaries_batch, reviews_batch, summaries_lengths, reviews_lengths) in enumerate(\n",
    "                    get_batches(sorted_summaries_short, sorted_reviews_short, batch_size)):\n",
    "                start_time = time.time()\n",
    "                _, loss = sess.run(\n",
    "                    [train_operation, cost],\n",
    "                    {inputs: reviews_batch,\n",
    "                     targets: summaries_batch,\n",
    "                     learning_rate: lr,\n",
    "                     target_seq_len: summaries_lengths,\n",
    "                     source_seq_len: reviews_lengths,\n",
    "                     keep_probability: keep_prob})\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(sorted_reviews_short) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n",
    "\n",
    "\n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "#             learning_rate *= learning_rate_decay\n",
    "#             if learning_rate < min_learning_rate:\n",
    "#                 learning_rate = min_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickled_data():\n",
    "    word_dicts_path = './checkpointed_data/word_dicts.p'\n",
    "    model_input_data_path = './checkpointed_data/model_input_data.p'\n",
    "    vocab_to_int, int_to_vocab, word_embedding_matrix = pickle.load(open(word_dicts_path, mode='rb'))\n",
    "    sorted_summaries, sorted_reviews = pickle.load(open(model_input_data_path, mode='rb'))\n",
    "    return vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews = load_pickled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building the graph!\n",
      "The shortest review length: 25\n",
      "The longest review length: 31\n",
      "Epoch   1/100 Batch   10/781 - Loss:  6.114, Seconds: 25.42\n",
      "Epoch   1/100 Batch   20/781 - Loss:  2.922, Seconds: 29.24\n",
      "Epoch   1/100 Batch   30/781 - Loss:  2.828, Seconds: 29.64\n",
      "Epoch   1/100 Batch   40/781 - Loss:  2.979, Seconds: 31.51\n",
      "Epoch   1/100 Batch   50/781 - Loss:  3.037, Seconds: 25.03\n",
      "Epoch   1/100 Batch   60/781 - Loss:  2.771, Seconds: 25.00\n",
      "Epoch   1/100 Batch   70/781 - Loss:  2.649, Seconds: 28.79\n",
      "Epoch   1/100 Batch   80/781 - Loss:  2.878, Seconds: 30.26\n",
      "Epoch   1/100 Batch   90/781 - Loss:  2.821, Seconds: 28.81\n",
      "Epoch   1/100 Batch  100/781 - Loss:  2.830, Seconds: 27.29\n",
      "Epoch   1/100 Batch  110/781 - Loss:  2.702, Seconds: 26.47\n",
      "Epoch   1/100 Batch  120/781 - Loss:  2.737, Seconds: 26.52\n",
      "Epoch   1/100 Batch  130/781 - Loss:  2.566, Seconds: 23.07\n",
      "Epoch   1/100 Batch  140/781 - Loss:  2.678, Seconds: 26.85\n",
      "Epoch   1/100 Batch  150/781 - Loss:  2.684, Seconds: 30.22\n",
      "Epoch   1/100 Batch  160/781 - Loss:  2.585, Seconds: 30.38\n",
      "Epoch   1/100 Batch  170/781 - Loss:  2.654, Seconds: 28.28\n",
      "Epoch   1/100 Batch  180/781 - Loss:  2.447, Seconds: 28.55\n",
      "Epoch   1/100 Batch  190/781 - Loss:  2.756, Seconds: 25.33\n",
      "Epoch   1/100 Batch  200/781 - Loss:  2.523, Seconds: 25.01\n",
      "Epoch   1/100 Batch  210/781 - Loss:  2.552, Seconds: 27.50\n",
      "Epoch   1/100 Batch  220/781 - Loss:  2.688, Seconds: 26.59\n",
      "Epoch   1/100 Batch  230/781 - Loss:  2.565, Seconds: 30.55\n",
      "Epoch   1/100 Batch  240/781 - Loss:  2.555, Seconds: 21.13\n",
      "Epoch   1/100 Batch  250/781 - Loss:  2.492, Seconds: 27.39\n",
      "Average loss for this update: 2.822\n",
      "New Record!\n",
      "Epoch   1/100 Batch  260/781 - Loss:  2.314, Seconds: 27.48\n",
      "Epoch   1/100 Batch  270/781 - Loss:  2.331, Seconds: 26.16\n",
      "Epoch   1/100 Batch  280/781 - Loss:  2.210, Seconds: 30.98\n",
      "Epoch   1/100 Batch  290/781 - Loss:  2.298, Seconds: 26.45\n",
      "Epoch   1/100 Batch  300/781 - Loss:  2.420, Seconds: 26.91\n",
      "Epoch   1/100 Batch  310/781 - Loss:  2.393, Seconds: 22.86\n",
      "Epoch   1/100 Batch  320/781 - Loss:  2.483, Seconds: 27.13\n",
      "Epoch   1/100 Batch  330/781 - Loss:  2.628, Seconds: 23.07\n",
      "Epoch   1/100 Batch  340/781 - Loss:  2.650, Seconds: 21.03\n",
      "Epoch   1/100 Batch  350/781 - Loss:  2.446, Seconds: 21.08\n",
      "Epoch   1/100 Batch  360/781 - Loss:  2.340, Seconds: 31.32\n",
      "Epoch   1/100 Batch  370/781 - Loss:  2.424, Seconds: 22.93\n",
      "Epoch   1/100 Batch  380/781 - Loss:  2.337, Seconds: 30.84\n",
      "Epoch   1/100 Batch  390/781 - Loss:  2.197, Seconds: 26.79\n",
      "Epoch   1/100 Batch  400/781 - Loss:  2.180, Seconds: 32.98\n",
      "Epoch   1/100 Batch  410/781 - Loss:  2.062, Seconds: 25.84\n",
      "Epoch   1/100 Batch  420/781 - Loss:  2.269, Seconds: 26.46\n",
      "Epoch   1/100 Batch  430/781 - Loss:  2.200, Seconds: 32.47\n",
      "Epoch   1/100 Batch  440/781 - Loss:  2.114, Seconds: 33.59\n",
      "Epoch   1/100 Batch  450/781 - Loss:  2.298, Seconds: 32.93\n",
      "Epoch   1/100 Batch  460/781 - Loss:  2.504, Seconds: 29.15\n",
      "Epoch   1/100 Batch  470/781 - Loss:  2.341, Seconds: 31.21\n",
      "Epoch   1/100 Batch  480/781 - Loss:  2.462, Seconds: 25.78\n",
      "Epoch   1/100 Batch  490/781 - Loss:  2.367, Seconds: 30.13\n",
      "Epoch   1/100 Batch  500/781 - Loss:  2.218, Seconds: 32.92\n",
      "Epoch   1/100 Batch  510/781 - Loss:  2.290, Seconds: 29.61\n",
      "Average loss for this update: 2.337\n",
      "New Record!\n",
      "Epoch   1/100 Batch  520/781 - Loss:  2.250, Seconds: 29.03\n",
      "Epoch   1/100 Batch  530/781 - Loss:  2.187, Seconds: 31.42\n",
      "Epoch   1/100 Batch  540/781 - Loss:  2.117, Seconds: 23.25\n",
      "Epoch   1/100 Batch  550/781 - Loss:  2.113, Seconds: 32.47\n",
      "Epoch   1/100 Batch  560/781 - Loss:  2.124, Seconds: 29.46\n",
      "Epoch   1/100 Batch  570/781 - Loss:  2.278, Seconds: 27.25\n",
      "Epoch   1/100 Batch  580/781 - Loss:  2.153, Seconds: 26.97\n",
      "Epoch   1/100 Batch  590/781 - Loss:  2.144, Seconds: 34.07\n",
      "Epoch   1/100 Batch  600/781 - Loss:  2.387, Seconds: 24.14\n",
      "Epoch   1/100 Batch  610/781 - Loss:  2.411, Seconds: 31.69\n",
      "Epoch   1/100 Batch  620/781 - Loss:  2.566, Seconds: 31.27\n",
      "Epoch   1/100 Batch  630/781 - Loss:  2.322, Seconds: 27.38\n",
      "Epoch   1/100 Batch  640/781 - Loss:  2.295, Seconds: 32.63\n",
      "Epoch   1/100 Batch  650/781 - Loss:  2.333, Seconds: 25.42\n",
      "Epoch   1/100 Batch  660/781 - Loss:  2.150, Seconds: 27.96\n",
      "Epoch   1/100 Batch  670/781 - Loss:  2.088, Seconds: 25.31\n",
      "Epoch   1/100 Batch  680/781 - Loss:  2.086, Seconds: 25.83\n",
      "Epoch   1/100 Batch  690/781 - Loss:  2.066, Seconds: 30.44\n",
      "Epoch   1/100 Batch  700/781 - Loss:  1.979, Seconds: 34.41\n",
      "Epoch   1/100 Batch  710/781 - Loss:  1.914, Seconds: 30.54\n",
      "Epoch   1/100 Batch  720/781 - Loss:  2.281, Seconds: 29.29\n",
      "Epoch   1/100 Batch  730/781 - Loss:  2.431, Seconds: 25.69\n",
      "Epoch   1/100 Batch  740/781 - Loss:  2.405, Seconds: 24.17\n",
      "Epoch   1/100 Batch  750/781 - Loss:  2.206, Seconds: 27.60\n",
      "Epoch   1/100 Batch  760/781 - Loss:  2.426, Seconds: 24.45\n",
      "Epoch   1/100 Batch  770/781 - Loss:  2.101, Seconds: 32.58\n",
      "Average loss for this update: 2.218\n",
      "New Record!\n",
      "Epoch   1/100 Batch  780/781 - Loss:  2.089, Seconds: 29.11\n",
      "Epoch   2/100 Batch   10/781 - Loss:  2.433, Seconds: 24.76\n",
      "Epoch   2/100 Batch   20/781 - Loss:  2.031, Seconds: 29.55\n",
      "Epoch   2/100 Batch   30/781 - Loss:  2.018, Seconds: 29.35\n",
      "Epoch   2/100 Batch   40/781 - Loss:  2.191, Seconds: 31.90\n",
      "Epoch   2/100 Batch   50/781 - Loss:  2.212, Seconds: 26.58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-9e2a272f1fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0msorted_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                       sorted_reviews)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-d40f8b19edfb>\u001b[0m in \u001b[0;36mbuild_and_train_model\u001b[0;34m(word_embedding_matrix, rnn_size, num_layers, keep_probability, vocab_to_int, batch_size, sorted_summaries, sorted_reviews)\u001b[0m\n\u001b[1;32m     85\u001b[0m                      \u001b[0mtarget_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                      \u001b[0msource_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreviews_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                      keep_probability: keep_prob})\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "build_and_train_model(word_embedding_matrix, \n",
    "                      rnn_size,\n",
    "                      num_layers,\n",
    "                      keep_prob,\n",
    "                      vocab_to_int,\n",
    "                      batch_size,\n",
    "                      sorted_summaries,\n",
    "                      sorted_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model results\n",
    "Note -- I was training this on my Macbook Pro but switched to an AWS EC2 instance with a GPU to train faster. The final results I got are below (this is the model being used in the summary generation below)\n",
    "\n",
    "<img src=\"images/ec2loss.png\"><br>\n",
    "<center><i>Results after training on GPU</i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing out summary generation\n",
    "Here's a function that takes in an input review and gives us back the generated summary from the model, so we can see what kind of summaries it comes out with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import text_cleaning\n",
    "\n",
    "def create_summary(input_review):\n",
    "    \n",
    "    # Clean the text to get it ready for the model\n",
    "    text = text_cleaning.clean_text(input_review)\n",
    "    text_int = [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n",
    "    \n",
    "    # Load in the model from the checkpoint\n",
    "    checkpoint = \"./model_checkpoints/best_model.ckpt\"\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "        loader.restore(sess, checkpoint)\n",
    "        \n",
    "        # The input tensors for inference\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        inference_logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        source_seq_len = loaded_graph.get_tensor_by_name('source_seq_len:0')\n",
    "        target_seq_len = loaded_graph.get_tensor_by_name('target_seq_len:0')\n",
    "        keep_probability = loaded_graph.get_tensor_by_name('keep_probability:0')\n",
    "        \n",
    "        # Run the graph to get the summary logits\n",
    "        summary_logits = sess.run(inference_logits, { input_data: [text_int] * batch_size,\n",
    "                                                      target_seq_len: [np.random.randint(5,8)],\n",
    "                                                      source_seq_len: [len(text_int)] * batch_size,\n",
    "                                                      keep_probability: 1.0\n",
    "                                                    })\n",
    "        # This returns a batch_size - length matrix of logits, so we just need the first one\n",
    "        summary_logits = summary_logits[0]\n",
    "        \n",
    "        # Convert back to human text to print out\n",
    "        pad = vocab_to_int[\"<PAD>\"]\n",
    "        \n",
    "        return \" \".join([int_to_vocab[i] for i in summary_logits if i != pad])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can try it out on sample reviews (note: in practical cases we'd send in all the reviews as a batch instead of iterating through each one). They're not perfect, but quite neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "['best cereal ever', 'best chocolate ever', 'best jerky ever']\n",
      "Took 28.24122966499999 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "sample_reviews = [\"It's become my favorite oatmeal, I even eat it for dinner from time to time...\",\n",
    "                  \"The chocolate was splendid, tasty. Highly recommended\",\n",
    "                  \"Absolutely the worst cheese I've had. Completely rotten, don't buy this!\"]\n",
    "\n",
    "summaries = [create_summary(review) for review in sample_reviews]\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print([summary for summary in summaries])\n",
    "print(\"Took {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model for serving\n",
    "Since we're going to use this model for inference in a production app (the Sumz chrome extension), we need to export it. Here's a great [tutorial](https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc) on how to go about this, which I'm following below.\n",
    "\n",
    "The gist is that we want to export just what we need for inference -- this significantly pares down the bloat in the model so we can serve it easier in production. We really just want the model and its weights in one file; essentially we want to freeze the model.\n",
    "\n",
    "The <b>checkpoints</b> we were saving above while training look like this:\n",
    "<img src=\"images/checkpoints_files.png\"><br>\n",
    "These are the following:\n",
    "* <b>best_model.ckpt.meta</b> This is holding the graph and metadata\n",
    "* <b>best_model.ckpt.index</b> Immutable key-value table that links each serialized tensor name and where to find it in the .data files\n",
    "* <b>best_model.ckpt.data-00000-of-00001</b> Holds the weights of the model\n",
    "* <b>checkpoint</b> High-level helper for loading different checkpoint files\n",
    "\n",
    "For production purposes we can 'freeze' the meta-graph, restore the weights. We'll also only need whatever is necessary for inference, which the following functino helps us do.\n",
    "\n",
    "The serialized frozen model is saved as a [ProtoBuf](https://developers.google.com/protocol-buffers/) file (a super compressed way to save the model and reload it); this is the final frozen model we'll use to serve to real users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/best_model.ckpt\n",
      "INFO:tensorflow:Froze 15 variables.\n",
      "Converted 15 variables to const ops.\n",
      "529 ops in final graph\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = './models/frozen_seq2seq_model.pb'\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    loader = tf.train.import_meta_graph('./model_checkpoints/best_model.ckpt.meta')\n",
    "    loader.restore(sess, './model_checkpoints/best_model.ckpt')\n",
    "\n",
    "    # Print out the operation names if needed\n",
    "    # ops = sess.graph.get_operations()\n",
    "#     for op in ops:\n",
    "#         print(op.name)\n",
    "\n",
    "    # We want the just the predictions operation (and all necessary variables for that)\n",
    "    # This converts those variables to constants (freezing them)\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n",
    "                                                                    tf.get_default_graph().as_graph_def(),\n",
    "                                                                    output_node_names=[\"predictions\"])\n",
    "    output_graph = MODEL_PATH\n",
    "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "    print(\"%d ops in final graph\" % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-loading the frozen model in\n",
    "Now let's try reloading the model in and using it for inference on the same reviews above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_graph():\n",
    "    with tf.gfile.GFile(MODEL_PATH, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph\n",
    "\n",
    "def summary_from_frozen_model(input_review):\n",
    "\n",
    "    text = text_cleaning.clean_text(input_review)\n",
    "    text_int = [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n",
    "    graph = load_graph()\n",
    "\n",
    "#     for op in graph.get_operations():\n",
    "#         print(op.name)\n",
    "\n",
    "    batch_size = 64 # Match original batch size    \n",
    "    input_data = graph.get_tensor_by_name('import/input:0')\n",
    "    target_seq_len = graph.get_tensor_by_name('import/target_seq_len:0')\n",
    "    source_seq_len = graph.get_tensor_by_name('import/source_seq_len:0')\n",
    "    keep_probability = graph.get_tensor_by_name('import/keep_probability:0')\n",
    "    inference_logits = graph.get_tensor_by_name('import/predictions:0')\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        feed = {\n",
    "            input_data: [text_int]*batch_size, \n",
    "            target_seq_len: [np.random.randint(5,8)], \n",
    "            source_seq_len: [len(text_int)]*batch_size,\n",
    "            keep_probability: 1.0}\n",
    "        summary_logits = sess.run(inference_logits, feed_dict = feed)[0]\n",
    "    \n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "    return \" \".join([int_to_vocab[i] for i in summary_logits if i != pad])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best cereal ever', 'best chocolate ever', 'best jerky ever']\n",
      "Took 11.593751017935574 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "sample_reviews = [\"It's become my favorite oatmeal, I even eat it for dinner from time to time...\",\n",
    "                  \"The chocolate was splendid, tasty. Highly recommended\",\n",
    "                  \"Absolutely the worst cheese I've had. Completely rotten, don't buy this!\"]\n",
    "\n",
    "summaries = [summary_from_frozen_model(review) for review in sample_reviews]\n",
    "\n",
    "print([summary for summary in summaries])\n",
    "\n",
    "end = timer()\n",
    "print(\"Took {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You can find the model working through the Sumz chrome extension (link); clearly it's not perfect at its summaries but it's cool that it can have some utility.\n",
    "\n",
    "Importantly I got to cover some of the concepts I wanted to learn more about, including:\n",
    "* Sequence to sequence models\n",
    "* Attention Mechanisms\n",
    "* Serving the model in a production environment (exporting)\n",
    "\n",
    "There are definitely some improvements to be made:\n",
    "* <b>Better data</b>: this model uses a fine foods dataset, and only a small portion of it. It'd be great to train it was a much bigger Amazon reviews set like [SNAP](https://snap.stanford.edu/data/web-Amazon.html) (34M reviews)\n",
    "* <b>Fine tuning on the model</b>: Especially the Attention Mechanism, and other architectures\n",
    "* <b>Bucketing</b>The review lengths vary wildly for Amazon reviews; having buckets could really help (putting reviews within a certain length in one bucket) so that the model can be trained based on that. Also having more types of products in the reviews could help inform the model in terms of summaries\n",
    "\n",
    "Hope this was helpful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
