{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence Tensorflow model for Amazon reviews\n",
    "\n",
    "This notebook walks through training a [Sequence to sequence model](https://www.tensorflow.org/tutorials/seq2seq) with Tensorflow (version 1.1).\n",
    "\n",
    "The model is currently used as the predictive backend for the SUMZ chrome extension, which takes in Amazon reviews on the current web page and displays a small summary of each review. The model is trained on the the [Amazon fine food reviews dataset.](https://www.kaggle.com/snap/amazon-fine-food-reviews) from Kaggle, which consists of 568K review-summary pairs.\n",
    "\n",
    "This notebook goes through the following:\n",
    "- Building a sequence-to-sequence model using Tensorflow\n",
    "- Using the preprocessed data from the data_preprocessing notebook to train the model\n",
    "- Exporting the model into ProtoBuff format for serving in a production environment\n",
    "\n",
    "This builds on the [Text Summarization](https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews) project by David Currie (this [Medium post](https://medium.com/towards-data-science/text-summarization-with-amazon-reviews-41801c2210b) goes into excellent detail as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.contrib.rnn import GRUCell, LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_input_placeholders():\n",
    "    \"\"\"\n",
    "    Create model input placeholders\n",
    "    : return: placeholder tensors\n",
    "    \"\"\"    \n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None,None])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_probability = tf.placeholder(tf.float32, name='keep_probability')\n",
    "    target_seq_len = tf.placeholder(tf.int32, (None,), name='target_seq_len')\n",
    "    max_target_seq_len = tf.reduce_max(target_seq_len, name='max_target_seq_len')\n",
    "    source_seq_len = tf.placeholder(tf.int32, (None,), name='source_seq_len')\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_probability, target_seq_len, max_target_seq_len, source_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_encoder_input(input_data, word_embedding_matrix):\n",
    "    return tf.nn.embedding_lookup(word_embedding_matrix, input_data)\n",
    "\n",
    "def encoding_layer(encoder_inputs, rnn_size, \n",
    "                   source_seq_len, num_layers, \n",
    "                   keep_prob, \n",
    "                   encoder_style, \n",
    "                   base_cell):\n",
    "    \"\"\"\n",
    "    Works with bidirectional and regular (unidirectional RNN)\n",
    "    as specificed by the 'encoder_style' parameter that can be either\n",
    "    'bidirectional_rnn' or 'unidirectional_rnn'\n",
    "    \n",
    "    Also can be passed in either a LSTMCell or GRUCell for 'LSTMCell' param\n",
    "    \"\"\"\n",
    "\n",
    "    if encoder_style == 'unidirectional_rnn':\n",
    "        print(\"UNIDIRECTIONAL ENCODER\")\n",
    "        print(\"ENCODER BASE CELL IS\", base_cell)\n",
    "        def make_cell(rnn_size):\n",
    "            return tf.contrib.rnn.DropoutWrapper(base_cell(rnn_size), output_keep_prob=keep_prob)\n",
    "        enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "        enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, \n",
    "                                                  encoder_inputs, \n",
    "                                                  sequence_length=source_seq_len, \n",
    "                                                  dtype=tf.float32)\n",
    "        \n",
    "    else:\n",
    "        print(\"BIDRECTIONAL ENCODER\")\n",
    "        print(\"ENCODER BASE CELL IS\", base_cell)\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                fwCell = base_cell(rnn_size)\n",
    "                bwCell = base_cell(rnn_size)\n",
    "                single_rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(cell = fwCell,\n",
    "                                                                        output_keep_prob = keep_prob)\n",
    "                single_rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(cell = bwCell,\n",
    "                                                                         output_keep_prob = keep_prob)\n",
    "                enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(single_rnn_cell_forward,\n",
    "                                                                        single_rnn_cell_backward,\n",
    "                                                                        encoder_inputs,\n",
    "                                                                        source_seq_len,\n",
    "                                                                        dtype = tf.float32)\n",
    "        enc_output = tf.concat(enc_output, 2) # Concatenate both outputs together\n",
    "        \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "\n",
    "    # Remove the last word (integer) from each target sequence\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size,-1], [1,1])\n",
    "    \n",
    "    # Add the <GO> token to each target sequence\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedded_decoder_input(input_data, word_embedding_matrix):\n",
    "    return tf.nn.embedding_lookup(word_embedding_matrix, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_decoder_cell(rnn_size, \n",
    "                      num_layers, \n",
    "                      encoder_output, \n",
    "                      source_seq_len, \n",
    "                      keep_prob,\n",
    "                      batch_size,\n",
    "                      encoder_state, \n",
    "                      attention, \n",
    "                      base_cell):\n",
    "\n",
    "    \"\"\"\n",
    "    Works with either GRU or basic LSTM cells, as 'GRUCell' or 'BasicLSTM'\n",
    "    for the 'cell_style' parameter\n",
    "    \n",
    "    Also works with or without Attention mechanism, as specified by the\n",
    "    'attention' parameter\n",
    "    \n",
    "    [@TODO Allow different attention mechanisms for comparison]\n",
    "    \"\"\"    \n",
    "    print(\"DECODER BASE CELL\", base_cell)\n",
    "    if attention == True:\n",
    "        print(\"DECODER ATTENTOIN IS TRUE\")\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(base_cell(rnn_size), output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                                   encoder_output,\n",
    "                                                                   source_seq_len,\n",
    "                                                                   normalize=False,\n",
    "                                                                   name='BahdanauAttention')\n",
    "\n",
    "        dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                              attention_mechanism,\n",
    "                                                              rnn_size)\n",
    "\n",
    "        initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(encoder_state[0],\n",
    "                                                                        _zero_state_tensors(rnn_size, \n",
    "                                                                                            batch_size, \n",
    "                                                                                            tf.float32))\n",
    "    \n",
    "    else:\n",
    "        print(\"DECODER ATTENTION IS FALSE\")\n",
    "        def make_cell(rnn_size):\n",
    "            return tf.contrib.rnn.DropoutWrapper(base_cell(rnn_size), output_keep_prob=keep_prob)\n",
    "\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "        initial_state = encoder_state\n",
    "        \n",
    "    return dec_cell, initial_state \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(input_data,\n",
    "                   word_embedding_matrix,\n",
    "                   num_layers, \n",
    "                   rnn_size, \n",
    "                   keep_prob, \n",
    "                   encoder_output, \n",
    "                   source_seq_len,\n",
    "                   encoder_state,\n",
    "                   batch_size,\n",
    "                   vocab_size,\n",
    "                   target_seq_len,\n",
    "                   max_target_seq_len,\n",
    "                   vocab_to_int,\n",
    "                   attention,\n",
    "                   base_cell):\n",
    "    \n",
    "    decoder_embedded_input = embedded_decoder_input(input_data, word_embedding_matrix)\n",
    "    decoder_cell, initial_state = make_decoder_cell(rnn_size, \n",
    "                                                    num_layers, \n",
    "                                                    encoder_output, \n",
    "                                                    source_seq_len, \n",
    "                                                    keep_prob, \n",
    "                                                    batch_size,\n",
    "                                                    encoder_state,\n",
    "                                                    attention=attention,\n",
    "                                                    base_cell=base_cell\n",
    "                                                   )\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                        kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "    # Training\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embedded_input,\n",
    "                                                            sequence_length = target_seq_len,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer)\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                               output_time_major=False,\n",
    "                                                               impute_finished=True,\n",
    "                                                               maximum_iterations=max_target_seq_len)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True): # Reuse same params for inference\n",
    "        \n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), \n",
    "                               [batch_size], \n",
    "                               name='start_tokens')\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(word_embedding_matrix,\n",
    "                                                                    start_tokens,\n",
    "                                                                    vocab_to_int['<EOS>'])\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                              output_time_major=False,\n",
    "                                                              impute_finished=True,\n",
    "                                                              maximum_iterations=max_target_seq_len)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_seq2seq(input_data, \n",
    "                 word_embedding_matrix,\n",
    "                 rnn_size,\n",
    "                 source_seq_len,\n",
    "                 num_layers,\n",
    "                 keep_prob,\n",
    "                 target_data,\n",
    "                 vocab_to_int,\n",
    "                 batch_size,\n",
    "                 vocab_size,\n",
    "                 target_seq_len,\n",
    "                 max_target_seq_len,\n",
    "                 encoder_style,\n",
    "                 attention,\n",
    "                 base_cell\n",
    "                 ):\n",
    "    \n",
    "\n",
    "    \n",
    "    # Encoding layer\n",
    "    encoder_inputs = embedded_encoder_input(input_data, word_embedding_matrix)\n",
    "    encoder_output, encoder_state = encoding_layer(encoder_inputs, \n",
    "                                                   rnn_size, \n",
    "                                                   source_seq_len, \n",
    "                                                   num_layers, \n",
    "                                                   keep_prob,\n",
    "                                                   encoder_style=encoder_style,\n",
    "                                                   base_cell=base_cell)\n",
    "    \n",
    "    # Decoding layer\n",
    "    processed_decoder_input = process_decoder_input(target_data, \n",
    "                                                    vocab_to_int, \n",
    "                                                    batch_size)\n",
    "    training_logits, inference_logits = decoding_layer(processed_decoder_input,\n",
    "                                                       word_embedding_matrix,\n",
    "                                                       num_layers, \n",
    "                                                       rnn_size, \n",
    "                                                       keep_prob, \n",
    "                                                       encoder_output, \n",
    "                                                       source_seq_len,\n",
    "                                                       encoder_state,\n",
    "                                                       batch_size,\n",
    "                                                       vocab_size,\n",
    "                                                       target_seq_len,\n",
    "                                                       max_target_seq_len,\n",
    "                                                       vocab_to_int,\n",
    "                                                       attention=attention,\n",
    "                                                       base_cell=base_cell)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch_to_pad):\n",
    "    max_size = max([len(item) for item in batch_to_pad])\n",
    "    padded_batch = [item + [vocab_to_int['<PAD>']] * (max_size - len(item)) for item in batch_to_pad]\n",
    "    return padded_batch\n",
    "\n",
    "def get_batches(summaries, reviews, batch_size):\n",
    "    for batch_i in range(0, len(reviews)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        reviews_batch = reviews[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = pad_batch(summaries_batch)\n",
    "        pad_reviews_batch = pad_batch(reviews_batch)\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        pad_reviews_lengths = []\n",
    "        for review in pad_reviews_batch:\n",
    "            pad_reviews_lengths.append(len(review))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_reviews_batch, pad_summaries_lengths, pad_reviews_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "rnn_size = 256\n",
    "batch_size = 64\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_train_model(word_embedding_matrix, \n",
    "                rnn_size,\n",
    "                num_layers,\n",
    "                keep_probability,\n",
    "                vocab_to_int,\n",
    "                batch_size,\n",
    "                sorted_summaries,\n",
    "                sorted_reviews,\n",
    "                encoder_style='unidirectional_rnn',\n",
    "                attention=True,\n",
    "                base_cell=LSTMCell):\n",
    "    \n",
    "\n",
    "    # GRAPH BUILDING\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        \n",
    "        # Model inputs\n",
    "        inputs, targets, learning_rate, keep_probability, target_seq_len, max_target_seq_len, source_seq_len = model_input_placeholders()\n",
    "        \n",
    "        # Create final logits tensors\n",
    "        training_logits, inference_logits = full_seq2seq(tf.reverse(inputs, [-1]),\n",
    "                                                         word_embedding_matrix,\n",
    "                                                         rnn_size,\n",
    "                                                         source_seq_len,\n",
    "                                                         num_layers,\n",
    "                                                         keep_probability,\n",
    "                                                         targets,\n",
    "                                                         vocab_to_int,\n",
    "                                                         batch_size,\n",
    "                                                         len(vocab_to_int)+1,\n",
    "                                                         target_seq_len,\n",
    "                                                         max_target_seq_len,                  \n",
    "                                                         encoder_style=encoder_style,\n",
    "                                                         attention=attention,\n",
    "                                                         base_cell=base_cell)\n",
    "\n",
    "        training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "        inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        \n",
    "        masks = tf.sequence_mask(target_seq_len, max_target_seq_len, dtype=tf.float32, name='masks')\n",
    "        \n",
    "        # Set up optimizer\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            \n",
    "            cost = tf.contrib.seq2seq.sequence_loss(training_logits,\n",
    "                                                    targets,\n",
    "                                                    masks)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "            gradients = optimizer.compute_gradients(cost)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "            train_operation = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "     \n",
    "    \n",
    "    start = 200000\n",
    "    end = start + 50000\n",
    "    sorted_summaries_short = sorted_summaries[start:end]\n",
    "    sorted_reviews_short = sorted_reviews[start:end]\n",
    "    print(\"The shortest review length:\", len(sorted_reviews_short[0]))\n",
    "    print(\"The longest review length:\", len(sorted_reviews_short[-1]))\n",
    "    \n",
    "    display_step = 1 # Check training loss after every 20 batches\n",
    "    stop = 5 # Stop training if average loss doesn't decrease in this mean update_checks\n",
    "    per_epoch = 3 # Make 3 update checks per epoch\n",
    "    update_check = (len(sorted_reviews_short)//batch_size//per_epoch)-1\n",
    "\n",
    "    update_loss = 0 \n",
    "    batch_loss = 0\n",
    "    summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "#     checkpoint = \"./model_checkpoints/best_model.ckpt\" \n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch_i in range(1, epochs+1):\n",
    "            update_loss = 0\n",
    "            batch_loss = 0\n",
    "            for batch_i, (summaries_batch, reviews_batch, summaries_lengths, reviews_lengths) in enumerate(\n",
    "                    get_batches(sorted_summaries_short, sorted_reviews_short, batch_size)):\n",
    "                start_time = time.time()\n",
    "                _, loss = sess.run(\n",
    "                    [train_operation, cost],\n",
    "                    {inputs: reviews_batch,\n",
    "                     targets: summaries_batch,\n",
    "                     learning_rate: lr,\n",
    "                     target_seq_len: summaries_lengths,\n",
    "                     source_seq_len: reviews_lengths,\n",
    "                     keep_probability: keep_prob})\n",
    "\n",
    "                batch_loss += loss\n",
    "                update_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(sorted_reviews_short) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time*display_step))\n",
    "                    batch_loss = 0\n",
    "\n",
    "                if batch_i % update_check == 0 and batch_i > 0:\n",
    "                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                    summary_update_loss.append(update_loss)\n",
    "\n",
    "                    # If the update loss is at a new minimum, save the model\n",
    "                    if update_loss <= min(summary_update_loss):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        saver = tf.train.Saver() \n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "                    update_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickled_data():\n",
    "    word_dicts_path = './checkpointed_data/word_dicts.p'\n",
    "    model_input_data_path = './checkpointed_data/model_input_data.p'\n",
    "    vocab_to_int, int_to_vocab, word_embedding_matrix = pickle.load(open(word_dicts_path, mode='rb'))\n",
    "    sorted_summaries, sorted_reviews = pickle.load(open(model_input_data_path, mode='rb'))\n",
    "    return vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int, int_to_vocab, word_embedding_matrix, sorted_summaries, sorted_reviews = load_pickled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIDRECTIONAL ENCODER\n",
      "ENCODER BASE CELL IS <class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell'>\n",
      "DECODER BASE CELL <class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell'>\n",
      "DECODER ATTENTION IS FALSE\n",
      "The shortest review length: 25\n",
      "The longest review length: 31\n",
      "Epoch   1/100 Batch    1/781 - Loss: 18.757, Seconds: 2.27\n",
      "Epoch   1/100 Batch    2/781 - Loss:  4.784, Seconds: 2.25\n",
      "Epoch   1/100 Batch    3/781 - Loss:  5.388, Seconds: 1.88\n",
      "Epoch   1/100 Batch    4/781 - Loss:  3.469, Seconds: 2.85\n",
      "Epoch   1/100 Batch    5/781 - Loss:  4.003, Seconds: 2.43\n",
      "Epoch   1/100 Batch    6/781 - Loss:  3.387, Seconds: 2.68\n",
      "Epoch   1/100 Batch    7/781 - Loss:  3.970, Seconds: 2.31\n",
      "Epoch   1/100 Batch    8/781 - Loss:  3.237, Seconds: 2.29\n",
      "Epoch   1/100 Batch    9/781 - Loss:  2.823, Seconds: 2.67\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-15a10e049a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m                       \u001b[0mencoder_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bidirectional_rnn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                       \u001b[0mattention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                       base_cell=LSTMCell)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-276-9f8cde7159f5>\u001b[0m in \u001b[0;36mbuild_and_train_model\u001b[0;34m(word_embedding_matrix, rnn_size, num_layers, keep_probability, vocab_to_int, batch_size, sorted_summaries, sorted_reviews, encoder_style, attention, base_cell)\u001b[0m\n\u001b[1;32m     89\u001b[0m                      \u001b[0mtarget_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                      \u001b[0msource_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreviews_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                      keep_probability: keep_prob})\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ''' \n",
    "#     ENCODER STYLE:    UNIDIRECTIONAL\n",
    "#     LSTM CELL STYLE:  LSTMCell\n",
    "#     ATTENTION:        TRUE\n",
    "# '''\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(1)\n",
    "# build_and_train_model(word_embedding_matrix, \n",
    "#                       rnn_size,\n",
    "#                       num_layers,\n",
    "#                       keep_prob,\n",
    "#                       vocab_to_int,\n",
    "#                       batch_size,\n",
    "#                       sorted_summaries,\n",
    "#                       sorted_reviews,\n",
    "#                       encoder_style='unidirectional_rnn',\n",
    "#                       attention=True,\n",
    "#                       base_cell=LSTMCell)\n",
    "\n",
    "# ''' \n",
    "#     ENCODER STYLE:    UNIDIRECTIONAL\n",
    "#     LSTM CELL STYLE:  GRUCell\n",
    "#     ATTENTION:        TRUE\n",
    "# '''\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(1)\n",
    "# build_and_train_model(word_embedding_matrix, \n",
    "#                       rnn_size,\n",
    "#                       num_layers,\n",
    "#                       keep_prob,\n",
    "#                       vocab_to_int,\n",
    "#                       batch_size,\n",
    "#                       sorted_summaries,\n",
    "#                       sorted_reviews,\n",
    "#                       encoder_style='unidirectional_rnn',\n",
    "#                       attention=True,\n",
    "#                       base_cell=GRUCell)\n",
    "\n",
    "# ''' \n",
    "#     ENCODER STYLE:    BIDIRECTIONAL\n",
    "#     LSTM CELL STYLE:  LSTMCell\n",
    "#     ATTENTION:        TRUE\n",
    "# '''\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(1)\n",
    "# build_and_train_model(word_embedding_matrix, \n",
    "#                       rnn_size,\n",
    "#                       num_layers,\n",
    "#                       keep_prob,\n",
    "#                       vocab_to_int,\n",
    "#                       batch_size,\n",
    "#                       sorted_summaries,\n",
    "#                       sorted_reviews,\n",
    "#                       encoder_style='bidirectional_rnn',\n",
    "#                       attention=True,\n",
    "#                       base_cell=LSTMCell)\n",
    "\n",
    "\n",
    "\n",
    "# ''' \n",
    "#     ENCODER STYLE:    BIDIRECTIONAL\n",
    "#     LSTM CELL STYLE:  GRUCell\n",
    "#     ATTENTION:        TRUE\n",
    "# '''\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(1)\n",
    "# build_and_train_model(word_embedding_matrix, \n",
    "#                       rnn_size,\n",
    "#                       num_layers,\n",
    "#                       keep_prob,\n",
    "#                       vocab_to_int,\n",
    "#                       batch_size,\n",
    "#                       sorted_summaries,\n",
    "#                       sorted_reviews,\n",
    "#                       encoder_style='bidirectional_rnn',\n",
    "#                       attention=True,\n",
    "#                       base_cell=GRUCell)\n",
    "\n",
    "# ''' \n",
    "#     ENCODER STYLE:    UNIDIRECTIONAL\n",
    "#     LSTM CELL STYLE:  LSTMCell\n",
    "#     ATTENTION:        FALSE\n",
    "# '''\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(1)\n",
    "# build_and_train_model(word_embedding_matrix, \n",
    "#                       rnn_size,\n",
    "#                       num_layers,\n",
    "#                       keep_prob,\n",
    "#                       vocab_to_int,\n",
    "#                       batch_size,\n",
    "#                       sorted_summaries,\n",
    "#                       sorted_reviews,\n",
    "#                       encoder_style='unidirectional_rnn',\n",
    "#                       attention=False,\n",
    "#                       base_cell=LSTMCell)\n",
    "\n",
    "''' \n",
    "    ENCODER STYLE:    BIDIRECTIONAL\n",
    "    LSTM CELL STYLE:  LSTMCell\n",
    "    ATTENTION:        FALSE\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "build_and_train_model(word_embedding_matrix, \n",
    "                      rnn_size,\n",
    "                      num_layers,\n",
    "                      keep_prob,\n",
    "                      vocab_to_int,\n",
    "                      batch_size,\n",
    "                      sorted_summaries,\n",
    "                      sorted_reviews,\n",
    "                      encoder_style='bidirectional_rnn',\n",
    "                      attention=False,\n",
    "                      base_cell=LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
